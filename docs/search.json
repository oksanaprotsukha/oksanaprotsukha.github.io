[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Oksana Protsukha",
    "section": "",
    "text": "Useful resources:\nHow to create a website with Quatro\n\n\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  url = {https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nn.d. https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {About},\n  url = {https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n‚ÄúAbout.‚Äù n.d. https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Oksana Protsukha",
    "section": "",
    "text": "My projects\n-in progress-\n\n\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  url = {https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nn.d. https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html."
  },
  {
    "objectID": "posts/2023-11-06-my-first-post/index.html",
    "href": "posts/2023-11-06-my-first-post/index.html",
    "title": "My first blog post",
    "section": "",
    "text": "This is the start of my blog posts. More to follow\n\n\n\nCitationBibTeX citation:@online{protsukha2023,\n  author = {Protsukha, Oksana and Doe, Jane},\n  title = {My First Blog Post},\n  date = {2023-11-06},\n  url = {https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nProtsukha, Oksana, and Jane Doe. 2023. ‚ÄúMy First Blog\nPost.‚Äù November 6, 2023. https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html."
  },
  {
    "objectID": "posts/2023-12-12-thomas-fire/index.html",
    "href": "posts/2023-12-12-thomas-fire/index.html",
    "title": "Impact of the Thomas Fire on AQI in Santa Barbara County, CA (2017)",
    "section": "",
    "text": "Author: Oksana Protsukha\nGithub repository: thomasfire_aqi_analysis\n\n\nIn this notebook, we are going to analyze the impact of Thomas Fire on the air quality in Santa Barbara county in 2017. To do so, we will complete the following two tasks:  1. Visualize the Air Quality Index (AQI) in Santa Barbara County for 2017 and 2018. 2. Create a false color image to visualize the Thomas Fire scar.\n\n\nThomas Fire started on December 4, north of Santa Paula, CA, and was contained only on January 12, 2018. The fire, spread by the unusually strong and persistent Santa Ana winds, destroyed 281,893 acres (440 sq mi; 114,078 ha) and threatened neighborhoods throughout Santa Barbara and Ventura counties. During this period, air quality significantly degraded to levels considered ‚Äúvery unhealthy‚Äù (201-300) by EPA standards. You can read about AIQ at AirNow website. \nFalse Color Image allows visualizing objects on the images that were generated via remote sensing using the wavelengths that fall outside the visual light spectrum: - Near-infrared (NIR) - Shortwave infrared (SWIR) - Midwave Infrared (MIR) - Infrared (IR) - Thermal or longwave infrared (TIR or LWIR) Assignment of these wavelegnths to different colors helps to highlight certain characteristics of an object that may not be visible in a true-color image.\nI highly recommend NASA website to read more about the false color imagery: NASA Earth Observatory\n\n\n\nThe primary purpose of the notebook is to demonstrate environmental data analysis methods using both vector and raster data.\n\n\n\n\nPython libraries\n\nnumpy\npandas\npyplot and patches from matplotlib\nrioxarray\ngeopandas\nxarray\n\nJupyter Notebook\nGitHub\n\n\n\n\n\nData wrangling and exploration with Pandas\nGeospatial data wrangling with GeoPandas and Rioxarray\nTime series analysis\nMerging tabular and vector data\nVisualizing analysis results with a line plot\nCreating and customizing a map with raster and vector data\n\n\n\n\n\n\nAir Quality Index (AQI) data from the US Environmental Protection Agency[1] to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County [2].\n\n\n\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite [3]. Note: data should be used for visualization purposes only.\nInformation about Landsat bands from USGS:\n\nWhat are the band designations for the Landsat satellites?\nCommon Landsat Band Combinations\nHow do I use a scale factor with Landsat Level-2 science products?\n\n\n\n\nA prepared shapefile of fire perimeters in California during 2017. The complete file can be accessed in the CA state geoportal[4].\nData for datasets 2 and 3 is available on Google Drive in the file thomasfire_aqi_analysis.zip.\n\n\n\n\nThe final outputs of the analysis include:\nA Line Plot depicting the time series analysis of the Air Quality Index (AQI) in Santa Barbara for 2017 and 2018. \nA Map with an overlay of the Thomas Fire scar on the Santa Barbara land cover."
  },
  {
    "objectID": "posts/2023-12-12-thomas-fire/index.html#overview",
    "href": "posts/2023-12-12-thomas-fire/index.html#overview",
    "title": "Impact of the Thomas Fire on AQI in Santa Barbara County, CA (2017)",
    "section": "",
    "text": "In this notebook, we are going to analyze the impact of Thomas Fire on the air quality in Santa Barbara county in 2017. To do so, we will complete the following two tasks:  1. Visualize the Air Quality Index (AQI) in Santa Barbara County for 2017 and 2018. 2. Create a false color image to visualize the Thomas Fire scar.\n\n\nThomas Fire started on December 4, north of Santa Paula, CA, and was contained only on January 12, 2018. The fire, spread by the unusually strong and persistent Santa Ana winds, destroyed 281,893 acres (440 sq mi; 114,078 ha) and threatened neighborhoods throughout Santa Barbara and Ventura counties. During this period, air quality significantly degraded to levels considered ‚Äúvery unhealthy‚Äù (201-300) by EPA standards. You can read about AIQ at AirNow website. \nFalse Color Image allows visualizing objects on the images that were generated via remote sensing using the wavelengths that fall outside the visual light spectrum: - Near-infrared (NIR) - Shortwave infrared (SWIR) - Midwave Infrared (MIR) - Infrared (IR) - Thermal or longwave infrared (TIR or LWIR) Assignment of these wavelegnths to different colors helps to highlight certain characteristics of an object that may not be visible in a true-color image.\nI highly recommend NASA website to read more about the false color imagery: NASA Earth Observatory\n\n\n\nThe primary purpose of the notebook is to demonstrate environmental data analysis methods using both vector and raster data.\n\n\n\n\nPython libraries\n\nnumpy\npandas\npyplot and patches from matplotlib\nrioxarray\ngeopandas\nxarray\n\nJupyter Notebook\nGitHub\n\n\n\n\n\nData wrangling and exploration with Pandas\nGeospatial data wrangling with GeoPandas and Rioxarray\nTime series analysis\nMerging tabular and vector data\nVisualizing analysis results with a line plot\nCreating and customizing a map with raster and vector data\n\n\n\n\n\n\nAir Quality Index (AQI) data from the US Environmental Protection Agency[1] to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County [2].\n\n\n\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite [3]. Note: data should be used for visualization purposes only.\nInformation about Landsat bands from USGS:\n\nWhat are the band designations for the Landsat satellites?\nCommon Landsat Band Combinations\nHow do I use a scale factor with Landsat Level-2 science products?\n\n\n\n\nA prepared shapefile of fire perimeters in California during 2017. The complete file can be accessed in the CA state geoportal[4].\nData for datasets 2 and 3 is available on Google Drive in the file thomasfire_aqi_analysis.zip.\n\n\n\n\nThe final outputs of the analysis include:\nA Line Plot depicting the time series analysis of the Air Quality Index (AQI) in Santa Barbara for 2017 and 2018. \nA Map with an overlay of the Thomas Fire scar on the Santa Barbara land cover."
  },
  {
    "objectID": "posts/2023-12-12-thomas-fire/index.html#visualize-aqi-in-santa-barbara-county-during-thomas-fire-2017",
    "href": "posts/2023-12-12-thomas-fire/index.html#visualize-aqi-in-santa-barbara-county-during-thomas-fire-2017",
    "title": "Impact of the Thomas Fire on AQI in Santa Barbara County, CA (2017)",
    "section": "Visualize AQI in Santa Barbara County during Thomas Fire (2017)",
    "text": "Visualize AQI in Santa Barbara County during Thomas Fire (2017)\n\nImport Air quality data \nWe are going to import AIQ data by county for 2017 and 2018.\n\n\nCode\n# import Daily AQI by County data from url: Dataset 1\n# read in 2017 Daily AQI by County\nurl_aqi_17 = 'https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip'\naqi_17 = pd.read_csv(url_aqi_17)\n\n# read in 2018 Daily AQI by County\nurl_aqi_18 = 'https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip'\naqi_18 = pd.read_csv(url_aqi_18)\n\n\n\n\nPrepare Air quality data\n\n\nJoin dataframes\nWe currently have two separate dataframes for 2017 and 2018. However, in order to analyze AQI for the two years, we need to combine them by appending the rows from one dataframe on top of the other. We will use the pandas function pd.concat() to do so.\nImportant: in order to combine the two dataframes their columns have to match.\n\n\nCode\n%%capture\n\n# check whether the columns between the two datasets match\nprint(f'Do columns match?\\n {aqi_18.columns == aqi_17.columns} \\n')\n\n# check whether the column datatypes between the two datasets match\nprint(f'Do datatypes of the columns match?\\n{aqi_18.dtypes == aqi_17.dtypes}')\n\n\nNote: Since the column names and data types of the two datasets match exactly, we can combine them into a single dataframe. To ensure data quality, we will verify that the combined dataframe has a total number of rows equal to the sum of the records from the two separate datasets.\n\n\nCode\n%%capture\n\n# combine two datasets aqi_17 and aqi_18 in a single one\naqi = pd.concat([aqi_17, aqi_18])\n\n# check the total count of rows matches the expected number\nprint(f'Is the total number of rows in the combined dataframe as expected? {len(aqi) == (len(aqi_17)+len(aqi_18))}')\n\n# check the first 2 rows of the combined dataframe\naqi.head(2)\n\n\n\n\nData wrangling\n\nFilter dataframe\nFor our analysis we need only AQI data from the Santa Barbara county. Let‚Äôs create a new dataframeaqi_sb filtered to Santa Barbara county.\nAdditionally, we will remove unecessary columns state_name, county_name, state_code and county_code from the new dataframe and update the column names to snake_case üêç to simplify data manipulation.\n\n\nCode\n%%capture\n\n# update the column names case to snake_case\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')\n\n# select the records where county_name = 'Santa Barbara' from the combined dataset aqi\naqi_sb = aqi.loc[aqi.county_name == 'Santa Barbara'].copy()\n\n# remove the state_name, county_name, state_code and county_code columns from aqi_sb\naqi_sb = aqi_sb.drop(columns = ['state_name', 'county_name', 'state_code', 'county_code'])\n\n\n\n\nSet date column as the index\nIn our analysis, we will examine data over time. Having a date column in the datetime format, configured as an index, enables the use of built-in functions when working with dates.\n\n\nCode\n# check the data types of the columns\nprint(aqi_sb.dtypes)\n\n\ndate                         object\naqi                           int64\ncategory                     object\ndefining_parameter           object\ndefining_site                object\nnumber_of_sites_reporting     int64\ndtype: object\n\n\n‚ö†Ô∏è Date column has type object rather than datetime as expected.\nWe will make the following changes to facilitate working with dates:\n\nConvert the date column of aqi_sb to a datetime object.\nSet the date column as the index for aqi_sb dataframe.\n\n\n\nCode\n%%capture\n\n# update date column values of aqi_sb to datetype object\naqi_sb.date = pd.to_datetime(aqi_sb.date)\n      \n# update the index of aqi_sb to be the date column\naqi_sb = aqi_sb.set_index('date')\n\n# check column date is updated\nprint(aqi_sb.index)\n\n\n\n\nAdd a rolling average column\nOur current dataframe contains daily values of AQI, which can create short-term fluctuations and noise in data. We‚Äôll apply a 5-day rolling average to smooth out the data and improve the quality of the analysis. To achieve this we will create a new variable, five_day_average, using the pandas method rolling.\nNote: rolling() is a method for pandas.series that provides rolling window calculations. This is a lazy method (think groupby), we need to specify what we want to calculate over each window. It returns pd.Series as ouput.\nLet‚Äôs update and view first few rows of the final dataframe. We will use this dataframe to visualize AQI in Santa Barbara county during 2017 and 2018.\n\n\nCode\n# calculate 5-day rolling average AQI\n# the parameter '5D' indicates we want the window to be 5 days\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean().round(2) \n\n# final dataframe for visualization\nfinal_table = PrettyTable()\n# add columns to the table\nfinal_table.field_names = aqi_sb.columns.tolist()\n\n# subset_df = pd.concat([aqi_sb.head(3), aqi_sb.tail(3)])\n\n# Add rows to the table\nfor _, row in aqi_sb.head(5).iterrows():\n    final_table.add_row(row.tolist())\n\n#PrettyTable(aqi_sb.head(2))\nprint(final_table)\n\n\n+-----+----------+--------------------+---------------+---------------------------+------------------+\n| aqi | category | defining_parameter | defining_site | number_of_sites_reporting | five_day_average |\n+-----+----------+--------------------+---------------+---------------------------+------------------+\n|  39 |   Good   |       Ozone        |  06-083-4003  |             12            |       39.0       |\n|  36 |   Good   |       Ozone        |  06-083-4003  |             11            |       37.5       |\n|  71 | Moderate |        PM10        |  06-083-4003  |             12            |      48.67       |\n|  34 |   Good   |       Ozone        |  06-083-4003  |             13            |       45.0       |\n|  37 |   Good   |       Ozone        |  06-083-4003  |             12            |       43.4       |\n+-----+----------+--------------------+---------------+---------------------------+------------------+\n\n\n\n\n\nVisualize AQI output \nIn the next step, we will create a line plot with daily AQI and 5-day average AQI to visualize the impact of the Thomas Fire (December 2017) on air quality in Santa Barbara County.\n\n\nCode\n# plot both the daily AQI and the 5-day average\nfig, ax = plt.subplots()\n\naqi_sb.aqi.plot(ax=ax,\n                color = '#fecc5c')\naqi_sb.five_day_average.plot(ax=ax,\n                color = '#a50f15')\n\n# update axis\nax.set_title('Daily Average vs 5 day Average AQI in Santa Barbara (2017-2018)')\nax.set_xlabel('Period')\nax.set_ylabel('AQI')\nax.legend(['AQI Daily Average', 'AQI 5-day Average'])\n\n# annotate the data source\nax.annotate(\"Data: AirData Website File Download Page.\\nhttps://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed 28 Nov. 2023\",\n            xy=(0.5, -0.15), \n            fontsize=7, \n            xycoords='axes fraction',  \n            color='#555555',\n            ha='center',\n            va='top')  \n\n# adjust the botton space of the figure\nplt.subplots_adjust(bottom=0.01)\n\nplt.show\n\n\n&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;\n\n\n\n\n\n\nInterpretation\nFrom the graph we can see a significant increase in both the daily average and 5-day rolling average in December 2017 compared to the rest of the year, which coincides with the Thomas Fire event. During this period, the AQI exceeded 200, falling into the very unhealthy category. According to AQI standards, at this level, the risk of health effects is increased for everyone."
  },
  {
    "objectID": "posts/2023-12-12-thomas-fire/index.html#create-a-false-color-image-to-visualize-the-thomas-fire-scar",
    "href": "posts/2023-12-12-thomas-fire/index.html#create-a-false-color-image-to-visualize-the-thomas-fire-scar",
    "title": "Impact of the Thomas Fire on AQI in Santa Barbara County, CA (2017)",
    "section": "Create a false color image to visualize the Thomas Fire scar",
    "text": "Create a false color image to visualize the Thomas Fire scar\n\nImport geospatial data\nData for the following two datasets is available on Google Drive in the file: thomasfire_aqi_analysis.zip.\n\nImport Landsat geospatial data (netCDF)\n\n\nCode\n# use rioxarray library to import netCDF file with geospatial features\nfp_lsat = 'data/landsat8-2018-01-26-sb-simplified.nc'\nlsat = rioxr.open_rasterio(fp_lsat)\n\n\n\n\nImport California fire perimeter shape file\n\n\nCode\n# use geopandas to import shape file with California fire perimeters \ncalfire = gpd.read_file(os.path.join(os.getcwd(), 'data', \n                                     'California_Fire_Perimeters_2017',\n                                     'California_Fire_Perimeters_2017.shp'))  \n\n\n\n\n\nPrepare geospatial data \nFirst, let‚Äôs inspect geospatial datasets.\n\n\nCode\n%%capture\n\n# explore shape file\n\n# view shape and data type\nprint(f'calfire dataset shape: {calfire.shape}')\nprint(f'calfire dataset crs: {calfire.crs}')\n\n# view the first two records of imported shape file data\ncalfire.head(2)\n\n# visualize calfire data\ncalfire.plot(color = 'none')\n\n\n\nCreate a new dataframe with Thomas Fire scar geometries\nCalifornia fire perimeter dataset includes the list of all fires in California during 2017. For our analysis, we are interested only in Thomas fire that occured that year.\nTherefore, we need to filter the dataset to the rows with Thomas fire.\nNote: We only have one record with fire_name = \"THOMAS\" in the dataframe. This is the record that contains the geometries of the Thomas Fire scar.\n\n\nCode\n# update the column names case to snake_case\ncalfire.columns = calfire.columns.str.lower().str.replace(' ','_')\n\n# find the value that corresponds to Thomas fire\ncalfire[calfire.fire_name.str.contains('Thomas', case=False, na=False)]['fire_name'].unique()\n\n# create a new dataframe with Thomas fire details\ntf = calfire[calfire.fire_name == 'THOMAS']\n# keep only required fields\ntf = tf.loc[:, ['fire_name', 'gis_acres', 'shape_leng','shape_area','geometry']]\n\n\n\n\nReproject CRS\n‚ö†Ô∏è Important:\n\nIn order to overlay the Thomas Fire perimeter on the Santa Barbara County land cover, we need to ensure that the CRSs of the two datasets are the same. To achieve this, we will reproject the CRS of the tf dataset into the CRS of the lsat dataset.\nThe lsat dataset includes an additional dimension, the band, which makes it a 3-dimensional object. We are going to convert it into a 2-dimensional dataset by dropping the band dimension.\n\n\n\nCode\n%%capture\n\n# check that crs of the shape file and netCDF match\ncalfire.crs == lsat.rio.crs\n\n# update CRS of the shape file to lsat CRS\ntf = tf.to_crs(crs = lsat.rio.crs)\nprint(f'CRS of the lsat and tf datasets match: {tf.crs == lsat.rio.crs}')\n\n# drop an extra dimension \"band\" from lsat dataset\nlsat = lsat.squeeze().drop('band')\n# check that band dimension has been dropped\nprint(f'After squeeze:\\ndimensions {lsat.dims}\\ncoords: {lsat.coords}')\n\n\n\n\n\nCreate a false color image of Thomas Fire Scar\nIn the final step we will create a map showing the shortwave infrared/nir/ ed false color image together with the Thomas fire perimeter.\n\n\nCode\nfig, ax = plt.subplots()\n\n# remove axis\nax.axis('off')\n\n# plot Thomas fire perimeter\ntf.plot(ax=ax, color='none', edgecolor = '#483C32')\ntf_patch = mpatches.Patch(facecolor = 'none', \n                          edgecolor='#483C32', \n                          label='Thomas Fire Perimeter (2017)')\n\n# plot raster image of Santa Barbara area\nlsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(ax=ax, robust = True)\n\n# add legend\nax.legend(handles=[tf_patch], frameon=True, loc='upper right')\nax.set_title('Thomas Fire (Santa Barbara, 2017)')\n\n# annotate the data source\nax.annotate(\"Data: Microsoft Planetary Computer.https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed 28 Nov. 2023.\\nCalifornia Fire Perimeters (All).https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed 28 Nov. 2023.\",\n            xy=(0.5, -0.15),  # Move the annotation lower to leave space for the x-axis title\n            fontsize=7, \n            xycoords='axes fraction',  \n            color='#555555',\n            ha='center')  \n\n# adjust the botton space of the figure\nplt.subplots_adjust(bottom=0.01)\n\n# display map\nplt.show()\n\n\n\n\n\nCitations\n[1] AirData Website File Download Page. link. Accessed 28 Nov.¬†2023.\n[2] ‚ÄúThomas Fire.‚Äù Wikipedia, 9 Nov.¬†2023. Wikipedia, link.\n[3] Microsoft Planetary Computer. link. Accessed 28 Nov.¬†2023.\n[4] California Fire Perimeters (All). link. Accessed 28 Nov.¬†2023."
  },
  {
    "objectID": "posts/python_notebook_render/STAC-search.html",
    "href": "posts/python_notebook_render/STAC-search.html",
    "title": "ipynb rendered as html",
    "section": "",
    "text": "Code\nimport numpy as np\nimport geopandas as gpd\nimport rioxarray as rioxr\nimport matplotlib.pyplot as plt\n\nfrom shapely.geometry import Polygon\n\n# used to access STAC catalogs\nfrom pystac_client import Client\n# used fo sign items from the MPC STAC catalog\nimport planetary_computer\n\n# other libraries for nice outputs\nfrom IPython.display import Image\nCode\n## Access\nCode\n# access catalog\ncatalog = Client.open('https://planetarycomputer.microsoft.com/api/stac/v1',\n                      modifier = planetary_computer.sign_inplace)"
  },
  {
    "objectID": "posts/python_notebook_render/STAC-search.html#exploration",
    "href": "posts/python_notebook_render/STAC-search.html#exploration",
    "title": "ipynb rendered as html",
    "section": "Exploration",
    "text": "Exploration\n\n\nCode\n# metadata from the catalog\nprint('Title:', catalog.title)\nprint('Description:', catalog.description)\n\n\nTitle: Microsoft Planetary Computer STAC API\nDescription: Searchable spatiotemporal metadata describing Earth science datasets hosted by the Microsoft Planetary Computer\n\n\nWe can access the catalog‚Äôs collections by using get_collections() method\n\n\nCode\ncatalog.get_collections()\n\n\n&lt;generator object Client.get_collections at 0x7e78023f3cd0&gt;\n\n\nNotice: the output of get_collections() is generator.\nThis is a special kind of lazy object in Python over which you can loop over like a list. Unlike a list, the items in a generator do not exist in memory until you explicitely iterate over them or convert them to a list. Let‚Äôs try getting the collections from the catalog again:\n\n\nCode\n# get collections and print their names\ncollections = list(catalog.get_collections())\n\nprint('Number of collections: ', len(collections))\nprint('Collections IDs:')\n\nfor collection in collections:\n    print('-', collection.id)\n\n\nNumber of collections:  122\nCollections IDs:\n- daymet-annual-pr\n- daymet-daily-hi\n- 3dep-seamless\n- 3dep-lidar-dsm\n- fia\n- sentinel-1-rtc\n- gridmet\n- daymet-annual-na\n- daymet-monthly-na\n- daymet-annual-hi\n- daymet-monthly-hi\n- daymet-monthly-pr\n- gnatsgo-tables\n- hgb\n- cop-dem-glo-30\n- cop-dem-glo-90\n- goes-cmi\n- terraclimate\n- nasa-nex-gddp-cmip6\n- gpm-imerg-hhr\n- gnatsgo-rasters\n- 3dep-lidar-hag\n- 3dep-lidar-intensity\n- 3dep-lidar-pointsourceid\n- mtbs\n- noaa-c-cap\n- 3dep-lidar-copc\n- modis-64A1-061\n- alos-fnf-mosaic\n- 3dep-lidar-returns\n- mobi\n- landsat-c2-l2\n- era5-pds\n- chloris-biomass\n- kaza-hydroforecast\n- planet-nicfi-analytic\n- modis-17A2H-061\n- modis-11A2-061\n- daymet-daily-pr\n- 3dep-lidar-dtm-native\n- 3dep-lidar-classification\n- 3dep-lidar-dtm\n- gap\n- modis-17A2HGF-061\n- planet-nicfi-visual\n- gbif\n- modis-17A3HGF-061\n- modis-09A1-061\n- alos-dem\n- alos-palsar-mosaic\n- deltares-water-availability\n- modis-16A3GF-061\n- modis-21A2-061\n- us-census\n- jrc-gsw\n- deltares-floods\n- modis-43A4-061\n- modis-09Q1-061\n- modis-14A1-061\n- hrea\n- modis-13Q1-061\n- modis-14A2-061\n- sentinel-2-l2a\n- modis-15A2H-061\n- modis-11A1-061\n- modis-15A3H-061\n- modis-13A1-061\n- daymet-daily-na\n- nrcan-landcover\n- modis-10A2-061\n- ecmwf-forecast\n- noaa-mrms-qpe-24h-pass2\n- sentinel-1-grd\n- nasadem\n- io-lulc\n- landsat-c2-l1\n- drcog-lulc\n- chesapeake-lc-7\n- chesapeake-lc-13\n- chesapeake-lu\n- noaa-mrms-qpe-1h-pass1\n- noaa-mrms-qpe-1h-pass2\n- noaa-nclimgrid-monthly\n- goes-glm\n- usda-cdl\n- eclipse\n- esa-cci-lc\n- esa-cci-lc-netcdf\n- fws-nwi\n- usgs-lcmap-conus-v13\n- usgs-lcmap-hawaii-v10\n- noaa-climate-normals-tabular\n- noaa-climate-normals-netcdf\n- noaa-climate-normals-gridded\n- aster-l1t\n- cil-gdpcir-cc-by-sa\n- io-lulc-9-class\n- io-biodiversity\n- naip\n- noaa-cdr-sea-surface-temperature-whoi\n- noaa-cdr-ocean-heat-content\n- cil-gdpcir-cc0\n- cil-gdpcir-cc-by\n- noaa-cdr-sea-surface-temperature-whoi-netcdf\n- noaa-cdr-sea-surface-temperature-optimum-interpolation\n- modis-10A1-061\n- sentinel-5p-l2-netcdf\n- sentinel-3-olci-wfr-l2-netcdf\n- noaa-cdr-ocean-heat-content-netcdf\n- sentinel-3-synergy-aod-l2-netcdf\n- sentinel-3-synergy-v10-l2-netcdf\n- sentinel-3-olci-lfr-l2-netcdf\n- sentinel-3-sral-lan-l2-netcdf\n- sentinel-3-slstr-lst-l2-netcdf\n- sentinel-3-slstr-wst-l2-netcdf\n- sentinel-3-sral-wat-l2-netcdf\n- ms-buildings\n- sentinel-3-slstr-frp-l2-netcdf\n- sentinel-3-synergy-syn-l2-netcdf\n- sentinel-3-synergy-vgp-l2-netcdf\n- sentinel-3-synergy-vg1-l2-netcdf\n- esa-worldcover"
  },
  {
    "objectID": "posts/python_notebook_render/STAC-search.html#collection",
    "href": "posts/python_notebook_render/STAC-search.html#collection",
    "title": "ipynb rendered as html",
    "section": "Collection",
    "text": "Collection\nWe can select a single collection for exploration using the get_child() method for the catalog and the collection id aas the parameter.\n\n\nCode\nnaip_collection = catalog.get_child('naip')\nnaip_collection\n\n\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Collection\"\n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"naip\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n                \n            \n                \n                    \n        \n            description\n            \"The [National Agriculture Imagery Program](https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/) (NAIP) provides U.S.-wide, high-resolution aerial imagery, with four spectral bands (R, G, B, IR).  NAIP is administered by the [Aerial Field Photography Office](https://www.fsa.usda.gov/programs-and-services/aerial-photography/) (AFPO) within the [US Department of Agriculture](https://www.usda.gov/) (USDA).  Data are captured at least once every three years for each state.  This dataset represents NAIP data from 2010-present, in [cloud-optimized GeoTIFF](https://www.cogeo.org/) format.\n\"\n        \n    \n                \n            \n                \n                    \n        \n            links\n            [] 6 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"items\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip/items\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/geo+json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            rel\n            \"root\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            rel\n            \"license\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://www.fsa.usda.gov/help/policies-and-links/\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Public Domain\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            rel\n            \"describedby\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/dataset/naip\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Human readable dataset overview and reference\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            \n        \n            \n                \n        \n            rel\n            \"self\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            \n        \n            \n                \n        \n            rel\n            \"parent\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            stac_extensions\n            [] 2 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/item-assets/v1.0.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"https://stac-extensions.github.io/table/v1.2.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            item_assets\n            \n        \n            \n                \n        \n            image\n            \n        \n            \n                \n        \n            type\n            \"image/tiff; application=geotiff; profile=cloud-optimized\"\n        \n    \n            \n        \n            \n                \n        \n            roles\n            [] 1 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            title\n            \"RGBIR COG tile\"\n        \n    \n            \n        \n            \n                \n        \n            eo:bands\n            [] 4 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"Red\"\n        \n    \n            \n        \n            \n                \n        \n            common_name\n            \"red\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            name\n            \"Green\"\n        \n    \n            \n        \n            \n                \n        \n            common_name\n            \"green\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            name\n            \"Blue\"\n        \n    \n            \n        \n            \n                \n        \n            common_name\n            \"blue\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            name\n            \"NIR\"\n        \n    \n            \n        \n            \n                \n        \n            common_name\n            \"nir\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"near-infrared\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            metadata\n            \n        \n            \n                \n        \n            type\n            \"text/plain\"\n        \n    \n            \n        \n            \n                \n        \n            roles\n            [] 1 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"metadata\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            title\n            \"FGDC Metdata\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            thumbnail\n            \n        \n            \n                \n        \n            type\n            \"image/jpeg\"\n        \n    \n            \n        \n            \n                \n        \n            roles\n            [] 1 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"thumbnail\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Thumbnail\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            msft:region\n            \"westeurope\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:container\n            \"naip\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:storage_account\n            \"naipeuwest\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:short_description\n            \"NAIP provides US-wide, high-resolution aerial imagery.  This dataset includes NAIP images from 2010 to the present.\"\n        \n    \n                \n            \n                \n                    \n        \n            title\n            \"NAIP: National Agriculture Imagery Program\"\n        \n    \n                \n            \n                \n                    \n        \n            extent\n            \n        \n            \n                \n        \n            spatial\n            \n        \n            \n                \n        \n            bbox\n            [] 1 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            [] 4 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            -124.784\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            24.744\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            -66.951\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            49.346\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            temporal\n            \n        \n            \n                \n        \n            interval\n            [] 1 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            [] 2 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"2010-01-01T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2021-12-31T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            license\n            \"proprietary\"\n        \n    \n                \n            \n                \n                    \n        \n            keywords\n            [] 7 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"NAIP\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"Aerial\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"Imagery\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \"USDA\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            \"AFPO\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            \"Agriculture\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            \"United States\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            providers\n            [] 3 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"USDA Farm Service Agency\"\n        \n    \n            \n        \n            \n                \n        \n            roles\n            [] 2 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"producer\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"licensor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.fsa.usda.gov/programs-and-services/aerial-photography/imagery-programs/naip-imagery/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            name\n            \"Esri\"\n        \n    \n            \n        \n            \n                \n        \n            roles\n            [] 1 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.esri.com/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            name\n            \"Microsoft\"\n        \n    \n            \n        \n            \n                \n        \n            roles\n            [] 2 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"host\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"processor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://planetarycomputer.microsoft.com\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            summaries\n            \n        \n            \n                \n        \n            gsd\n            [] 2 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            0.6\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            1.0\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            eo:bands\n            [] 4 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"Red\"\n        \n    \n            \n        \n            \n                \n        \n            common_name\n            \"red\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"visible red\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            name\n            \"Green\"\n        \n    \n            \n        \n            \n                \n        \n            common_name\n            \"green\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"visible green\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            name\n            \"Blue\"\n        \n    \n            \n        \n            \n                \n        \n            common_name\n            \"blue\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"visible blue\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            name\n            \"NIR\"\n        \n    \n            \n        \n            \n                \n        \n            common_name\n            \"nir\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"near-infrared\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            assets\n            \n        \n            \n                \n        \n            thumbnail\n            \n        \n            \n                \n        \n            href\n            \"https://ai4edatasetspublicassets.blob.core.windows.net/assets/pc_thumbnails/naip.png\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"image/png\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"NAIP thumbnail\"\n        \n    \n            \n        \n            \n                \n        \n            roles\n            [] 1 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"thumbnail\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            geoparquet-items\n            \n        \n            \n                \n        \n            href\n            \"abfs://items/naip.parquet\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/x-parquet\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"GeoParquet STAC items\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Snapshot of the collection's STAC items exported to GeoParquet format.\"\n        \n    \n            \n        \n            \n                \n        \n            msft:partition_info\n            \n        \n            \n                \n        \n            is_partitioned\n            True\n        \n    \n            \n        \n            \n                \n        \n            partition_frequency\n            \"AS\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            table:storage_options\n            \n        \n            \n                \n        \n            account_name\n            \"pcstacitems\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            roles\n            [] 1 items\n        \n        \n            \n        \n            \n                \n        \n            0\n            \"stac-items\""
  },
  {
    "objectID": "posts/python_notebook_render/STAC-search.html#catalog-search",
    "href": "posts/python_notebook_render/STAC-search.html#catalog-search",
    "title": "ipynb rendered as html",
    "section": "Catalog search",
    "text": "Catalog search\nWe can narrow the search within the catalog by specifying a time range, an area of interest, and the collection name. The simplest way to define the area of interest to look in the catalog are:\n\na GeoJSON-type dictionary with the coordinates of the bounding box,\nas a list [xmin, ymin, xmax, ymax] with the coordinate values defining the four corners of the bounding box.\n\nYou could also use a point, or some more complex polygon.\nIn this lab we are going look for NAIP scenes over Santa Barbara from 2018 to 2020. We‚Äôll use GeoJSON method to define the area of interest:\n\n\nCode\n# temporal range of interest\ntime_range = \"2018-01-01/2023-01-01\"\n\n# NCEAS bounding box (as a GeoJSON)\nbbox = {\n    \"type\": \"Polygon\",\n    \"coordinates\":[\n        [\n            [-119.70608227128903, 34.426300194372274],\n            [-119.70608227128903, 34.42041139020533],\n            [-119.6967885126002, 34.42041139020533],\n            [-119.6967885126002, 34.426300194372274],\n            [-119.70608227128903, 34.426300194372274]\n        ]\n    ],\n}\n\n# catalog search\nsearch = catalog.search(\n    collections=['naip'],\n    intersects=bbox,\n    datetime=time_range)\nsearch\n\n\n&lt;pystac_client.item_search.ItemSearch at 0x17bc82810&gt;\n\n\nTo get the items found in the search (or check if there were any matches in the search) we use the item_collection() method:\n\n\nCode\nitems = search.item_collection()\n\n# number of items \nlen(items)\n\n\n2\n\n\n\n\nCode\nitems\n\n\n\n\n\n    \n    \n    \n        \n            \n                \n                    ItemCollection\n                \n            \n            \n\nItems\n\n\n\n\n\nItem: ca_m_3411935_sw_11_060_20200521\n\n\n\nid: ca_m_3411935_sw_11_060_20200521\n\n\nbbox: [-119.754272, 34.371741, -119.683292, 34.440724]\n\n\ngsd: 0.6\n\n\ndatetime: 2020-05-21T00:00:00Z\n\n\nnaip:year: 2020\n\n\nproj:bbox: [246930.0, 3806808.0, 253260.0, 3814296.0]\n\n\nproj:epsg: 26911\n\n\nnaip:state: ca\n\n\nproj:shape: [12480, 10550]\n\n\nproj:transform: [0.6, 0.0, 246930.0, 0.0, -0.6, 3814296.0, 0.0, 0.0, 1.0]\n\n\n\n\nSTAC Extensions\n\n\n\nhttps://stac-extensions.github.io/eo/v1.0.0/schema.json\n\n\nhttps://stac-extensions.github.io/projection/v1.0.0/schema.json\n\n\n\n\n\nAssets\n\n\n\n\n\nAsset: RGBIR COG tile\n\n\n\nhref: https://naipeuwest.blob.core.windows.net/naip/v002/ca/2020/ca_060cm_2020/34119/m_3411935_sw_11_060_20200521.tif?st=2023-11-26T21%3A20%3A49Z&se=2023-12-04T21%3A20%3A50Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-27T21%3A20%3A48Z&ske=2023-12-04T21%3A20%3A48Z&sks=b&skv=2021-06-08&sig=2kcLaAy5fiI4LOoyFNeWCIAwC0gi6/vTkqmGrUJntoY%3D\n\n\ntype: image/tiff; application=geotiff; profile=cloud-optimized\n\n\ntitle: RGBIR COG tile\n\n\nroles: ['data']\n\n\nowner: ca_m_3411935_sw_11_060_20200521\n\n\neo:bands: [{'name': 'Red', 'common_name': 'red'}, {'name': 'Green', 'common_name': 'green'}, {'name': 'Blue', 'common_name': 'blue'}, {'name': 'NIR', 'common_name': 'nir', 'description': 'near-infrared'}]\n\n\n\n\n\n\n\n\n\n\nAsset: Thumbnail\n\n\n\nhref: https://naipeuwest.blob.core.windows.net/naip/v002/ca/2020/ca_060cm_2020/34119/m_3411935_sw_11_060_20200521.200.jpg?st=2023-11-26T21%3A20%3A49Z&se=2023-12-04T21%3A20%3A50Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-27T21%3A20%3A48Z&ske=2023-12-04T21%3A20%3A48Z&sks=b&skv=2021-06-08&sig=2kcLaAy5fiI4LOoyFNeWCIAwC0gi6/vTkqmGrUJntoY%3D\n\n\ntype: image/jpeg\n\n\ntitle: Thumbnail\n\n\nroles: ['thumbnail']\n\n\nowner: ca_m_3411935_sw_11_060_20200521\n\n\n\n\n\n\n\n\n\n\nAsset: TileJSON with default rendering\n\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/tilejson.json?collection=naip&item=ca_m_3411935_sw_11_060_20200521&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png\n\n\ntype: application/json\n\n\ntitle: TileJSON with default rendering\n\n\nroles: ['tiles']\n\n\nowner: ca_m_3411935_sw_11_060_20200521\n\n\n\n\n\n\n\n\n\n\nAsset: Rendered preview\n\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/preview.png?collection=naip&item=ca_m_3411935_sw_11_060_20200521&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png\n\n\ntype: image/png\n\n\ntitle: Rendered preview\n\n\nroles: ['overview']\n\n\nowner: ca_m_3411935_sw_11_060_20200521\n\n\nrel: preview\n\n\n\n\n\n\n\nLinks\n\n\n\n\n\nLink:\n\n\n\nrel: collection\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip\n\n\ntype: application/json\n\n\n\n\n\n\n\n\n\n\nLink:\n\n\n\nrel: parent\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip\n\n\ntype: application/json\n\n\n\n\n\n\n\n\n\n\nLink: Microsoft Planetary Computer STAC API\n\n\n\nrel: root\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1\n\n\ntype: application/json\n\n\ntitle: Microsoft Planetary Computer STAC API\n\n\n\n\n\n\n\n\n\n\nLink:\n\n\n\nrel: self\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip/items/ca_m_3411935_sw_11_060_20200521\n\n\ntype: application/geo+json\n\n\n\n\n\n\n\n\n\n\nLink: Map of item\n\n\n\nrel: preview\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/map?collection=naip&item=ca_m_3411935_sw_11_060_20200521\n\n\ntype: text/html\n\n\ntitle: Map of item\n\n\n\n\n\n\n\n\n\n\n\n\n\nItem: ca_m_3411935_sw_11_060_20180724_20190209\n\n\n\nid: ca_m_3411935_sw_11_060_20180724_20190209\n\n\nbbox: [-119.753736, 34.372185, -119.683827, 34.44028]\n\n\ngsd: 0.6\n\n\ndatetime: 2018-07-24T00:00:00Z\n\n\nnaip:year: 2018\n\n\nproj:bbox: [246978.0, 3806856.0, 253212.0, 3814248.0]\n\n\nproj:epsg: 26911\n\n\nnaip:state: ca\n\n\nproj:shape: [12320, 10390]\n\n\nproj:transform: [0.6, 0.0, 246978.0, 0.0, -0.6, 3814248.0, 0.0, 0.0, 1.0]\n\n\n\n\nSTAC Extensions\n\n\n\nhttps://stac-extensions.github.io/eo/v1.0.0/schema.json\n\n\nhttps://stac-extensions.github.io/projection/v1.0.0/schema.json\n\n\n\n\n\nAssets\n\n\n\n\n\nAsset: RGBIR COG tile\n\n\n\nhref: https://naipeuwest.blob.core.windows.net/naip/v002/ca/2018/ca_060cm_2018/34119/m_3411935_sw_11_060_20180724_20190209.tif?st=2023-11-26T21%3A20%3A49Z&se=2023-12-04T21%3A20%3A50Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-27T21%3A20%3A48Z&ske=2023-12-04T21%3A20%3A48Z&sks=b&skv=2021-06-08&sig=2kcLaAy5fiI4LOoyFNeWCIAwC0gi6/vTkqmGrUJntoY%3D\n\n\ntype: image/tiff; application=geotiff; profile=cloud-optimized\n\n\ntitle: RGBIR COG tile\n\n\nroles: ['data']\n\n\nowner: ca_m_3411935_sw_11_060_20180724_20190209\n\n\neo:bands: [{'name': 'Red', 'common_name': 'red'}, {'name': 'Green', 'common_name': 'green'}, {'name': 'Blue', 'common_name': 'blue'}, {'name': 'NIR', 'common_name': 'nir', 'description': 'near-infrared'}]\n\n\n\n\n\n\n\n\n\n\nAsset: FGDC Metdata\n\n\n\nhref: https://naipeuwest.blob.core.windows.net/naip/v002/ca/2018/ca_fgdc_2018/34119/m_3411935_sw_11_060_20180724.txt?st=2023-11-26T21%3A20%3A49Z&se=2023-12-04T21%3A20%3A50Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-27T21%3A20%3A48Z&ske=2023-12-04T21%3A20%3A48Z&sks=b&skv=2021-06-08&sig=2kcLaAy5fiI4LOoyFNeWCIAwC0gi6/vTkqmGrUJntoY%3D\n\n\ntype: text/plain\n\n\ntitle: FGDC Metdata\n\n\nroles: ['metadata']\n\n\nowner: ca_m_3411935_sw_11_060_20180724_20190209\n\n\n\n\n\n\n\n\n\n\nAsset: Thumbnail\n\n\n\nhref: https://naipeuwest.blob.core.windows.net/naip/v002/ca/2018/ca_060cm_2018/34119/m_3411935_sw_11_060_20180724_20190209.200.jpg?st=2023-11-26T21%3A20%3A49Z&se=2023-12-04T21%3A20%3A50Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-27T21%3A20%3A48Z&ske=2023-12-04T21%3A20%3A48Z&sks=b&skv=2021-06-08&sig=2kcLaAy5fiI4LOoyFNeWCIAwC0gi6/vTkqmGrUJntoY%3D\n\n\ntype: image/jpeg\n\n\ntitle: Thumbnail\n\n\nroles: ['thumbnail']\n\n\nowner: ca_m_3411935_sw_11_060_20180724_20190209\n\n\n\n\n\n\n\n\n\n\nAsset: TileJSON with default rendering\n\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/tilejson.json?collection=naip&item=ca_m_3411935_sw_11_060_20180724_20190209&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png\n\n\ntype: application/json\n\n\ntitle: TileJSON with default rendering\n\n\nroles: ['tiles']\n\n\nowner: ca_m_3411935_sw_11_060_20180724_20190209\n\n\n\n\n\n\n\n\n\n\nAsset: Rendered preview\n\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/preview.png?collection=naip&item=ca_m_3411935_sw_11_060_20180724_20190209&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png\n\n\ntype: image/png\n\n\ntitle: Rendered preview\n\n\nroles: ['overview']\n\n\nowner: ca_m_3411935_sw_11_060_20180724_20190209\n\n\nrel: preview\n\n\n\n\n\n\n\nLinks\n\n\n\n\n\nLink:\n\n\n\nrel: collection\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip\n\n\ntype: application/json\n\n\n\n\n\n\n\n\n\n\nLink:\n\n\n\nrel: parent\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip\n\n\ntype: application/json\n\n\n\n\n\n\n\n\n\n\nLink: Microsoft Planetary Computer STAC API\n\n\n\nrel: root\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1\n\n\ntype: application/json\n\n\ntitle: Microsoft Planetary Computer STAC API\n\n\n\n\n\n\n\n\n\n\nLink:\n\n\n\nrel: self\n\n\nhref: https://planetarycomputer.microsoft.com/api/stac/v1/collections/naip/items/ca_m_3411935_sw_11_060_20180724_20190209\n\n\ntype: application/geo+json\n\n\n\n\n\n\n\n\n\n\nLink: Map of item\n\n\n\nrel: preview\n\n\nhref: https://planetarycomputer.microsoft.com/api/data/v1/item/map?collection=naip&item=ca_m_3411935_sw_11_060_20180724_20190209\n\n\ntype: text/html\n\n\ntitle: Map of item"
  },
  {
    "objectID": "posts/python_notebook_render/STAC-search.html#items",
    "href": "posts/python_notebook_render/STAC-search.html#items",
    "title": "ipynb rendered as html",
    "section": "Items",
    "text": "Items\nLet‚Äôs get the first item in the search\n\n\nCode\n# get the first item in the catalog search\nitem = items[0]\ntype(item)\n\n\npystac.item.Item\n\n\nRemember: the STAC item is the core object of the catalog. The item does not contain the data itself, but rather metadata about the item and links to access the actual data. Some of the metadata:\n\n\nCode\nprint('id', item.id)\nitem.properties\n\n\nid ca_m_3411935_sw_11_060_20200521\n\n\n{'gsd': 0.6,\n 'datetime': '2020-05-21T00:00:00Z',\n 'naip:year': '2020',\n 'proj:bbox': [246930.0, 3806808.0, 253260.0, 3814296.0],\n 'proj:epsg': 26911,\n 'naip:state': 'ca',\n 'proj:shape': [12480, 10550],\n 'proj:transform': [0.6, 0.0, 246930.0, 0.0, -0.6, 3814296.0, 0.0, 0.0, 1.0]}\n\n\n\n\nCode\nitem.assets\n\n\n{'image': &lt;Asset href=https://naipeuwest.blob.core.windows.net/naip/v002/ca/2020/ca_060cm_2020/34119/m_3411935_sw_11_060_20200521.tif?st=2023-11-28T21%3A45%3A03Z&se=2023-11-29T22%3A30%3A03Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-29T13%3A45%3A40Z&ske=2023-12-06T13%3A45%3A40Z&sks=b&skv=2021-06-08&sig=OU28gjf8cvXHXJ9yN4B4wAB5ILA7T%2B1BcRSa0SR2JLY%3D&gt;,\n 'thumbnail': &lt;Asset href=https://naipeuwest.blob.core.windows.net/naip/v002/ca/2020/ca_060cm_2020/34119/m_3411935_sw_11_060_20200521.200.jpg?st=2023-11-28T21%3A45%3A03Z&se=2023-11-29T22%3A30%3A03Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-29T13%3A45%3A40Z&ske=2023-12-06T13%3A45%3A40Z&sks=b&skv=2021-06-08&sig=OU28gjf8cvXHXJ9yN4B4wAB5ILA7T%2B1BcRSa0SR2JLY%3D&gt;,\n 'tilejson': &lt;Asset href=https://planetarycomputer.microsoft.com/api/data/v1/item/tilejson.json?collection=naip&item=ca_m_3411935_sw_11_060_20200521&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png&gt;,\n 'rendered_preview': &lt;Asset href=https://planetarycomputer.microsoft.com/api/data/v1/item/preview.png?collection=naip&item=ca_m_3411935_sw_11_060_20200521&assets=image&asset_bidx=image%7C1%2C2%2C3&format=png&gt;}\n\n\n\n\nCode\nfor key in item.assets.keys():\n    print(key, '--', item.assets[key].title)\n\n\nimage -- RGBIR COG tile\nthumbnail -- Thumbnail\ntilejson -- TileJSON with default rendering\nrendered_preview -- Rendered preview\n\n\n\n\nCode\nitem.assets.keys()\n\n\ndict_keys(['image', 'thumbnail', 'tilejson', 'rendered_preview'])\n\n\n\n\nCode\nitem.assets['image'].title\n\n\n'RGBIR COG tile'\n\n\nNotice each asset has an href, which is a link to the asset object (ie. the data). For example, we can use the URL for the rendered preview asset to plot it:\n\n\nCode\nImage(url=item.assets['rendered_preview'].href, width=500)"
  },
  {
    "objectID": "posts/python_notebook_render/STAC-search.html#load-data",
    "href": "posts/python_notebook_render/STAC-search.html#load-data",
    "title": "ipynb rendered as html",
    "section": "Load data",
    "text": "Load data\nthe raster data in our current item is in the image asset. Again, we access this data via url. This time we open it using rioxr.open_raster() directly:\n\n\nCode\nsb = rioxr.open_rasterio(item.assets['image'].href)\nsb\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 4, y: 12480, x: 10550)&gt;\n[526656000 values with dtype=uint8]\nCoordinates:\n  * band         (band) int64 1 2 3 4\n  * x            (x) float64 2.469e+05 2.469e+05 ... 2.533e+05 2.533e+05\n  * y            (y) float64 3.814e+06 3.814e+06 ... 3.807e+06 3.807e+06\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:             Area\n    TIFFTAG_IMAGEDESCRIPTION:  OrthoVista\n    TIFFTAG_RESOLUTIONUNIT:    1 (unitless)\n    TIFFTAG_SOFTWARE:          Trimble Germany GmbH\n    TIFFTAG_XRESOLUTION:       1\n    TIFFTAG_YRESOLUTION:       1\n    _FillValue:                0\n    scale_factor:              1.0\n    add_offset:                0.0xarray.DataArrayband: 4y: 12480x: 10550...[526656000 values with dtype=uint8]Coordinates: (4)band(band)int641 2 3 4array([1, 2, 3, 4])x(x)float642.469e+05 2.469e+05 ... 2.533e+05array([246930.3, 246930.9, 246931.5, ..., 253258.5, 253259.1, 253259.7])y(y)float643.814e+06 3.814e+06 ... 3.807e+06array([3814295.7, 3814295.1, 3814294.5, ..., 3806809.5, 3806808.9, 3806808.3])spatial_ref()int640crs_wkt :PROJCS[\"NAD83 / UTM zone 11N\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"26911\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314140356inverse_flattening :298.257222101reference_ellipsoid_name :GRS 1980longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :NAD83horizontal_datum_name :North American Datum 1983projected_crs_name :NAD83 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"NAD83 / UTM zone 11N\",GEOGCS[\"NAD83\",DATUM[\"North_American_Datum_1983\",SPHEROID[\"GRS 1980\",6378137,298.257222101,AUTHORITY[\"EPSG\",\"7019\"]],AUTHORITY[\"EPSG\",\"6269\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4269\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"26911\"]]GeoTransform :246930.0 0.6 0.0 3814296.0 0.0 -0.6array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1, 2, 3, 4], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([          246930.3,           246930.9,           246931.5,\n       246932.09999999998, 246932.69999999998,           246933.3,\n                 246933.9,           246934.5, 246935.09999999998,\n       246935.69999999998,\n       ...\n                 253254.3,           253254.9,           253255.5,\n       253256.09999999998, 253256.69999999998,           253257.3,\n                 253257.9,           253258.5, 253259.09999999998,\n       253259.69999999998],\n      dtype='float64', name='x', length=10550))yPandasIndexPandasIndex(Index([         3814295.7,          3814295.1,          3814294.5,\n       3814293.9000000004, 3814293.3000000003,          3814292.7,\n                3814292.1,          3814291.5, 3814290.9000000004,\n       3814290.3000000003,\n       ...\n                3806813.7,          3806813.1,          3806812.5,\n       3806811.9000000004, 3806811.3000000003,          3806810.7,\n                3806810.1,          3806809.5, 3806808.9000000004,\n       3806808.3000000003],\n      dtype='float64', name='y', length=12480))Attributes: (9)AREA_OR_POINT :AreaTIFFTAG_IMAGEDESCRIPTION :OrthoVistaTIFFTAG_RESOLUTIONUNIT :1 (unitless)TIFFTAG_SOFTWARE :Trimble Germany GmbHTIFFTAG_XRESOLUTION :1TIFFTAG_YRESOLUTION :1_FillValue :0scale_factor :1.0add_offset :0.0\n\n\n\n\nCode\n# plot raster with correct ratio\nsize = 6 # height in in of plot height\naspect = sb.rio.width / sb.rio.height \n# select R,G,B bands and plot\nsb.sel(band=[1,2,3]).plot.imshow(size=size, aspect=aspect)"
  },
  {
    "objectID": "posts/python_notebook_render/STAC-search.html#exercise",
    "href": "posts/python_notebook_render/STAC-search.html#exercise",
    "title": "ipynb rendered as html",
    "section": "Exercise",
    "text": "Exercise\nThe cop-dem-glo-90 (id of the collection) contains the Copernicus DEM at 90m resolution (the one we used for Grand Canyon).\n\nReuse the bbox for Santa Barbara to look for items in this collection.\nGet the first item in the search and check its assets.\nCheck the item‚Äôs rendered preview asset by clicking on it‚Äôs URL.\nOpen the item‚Äôs data using rioxarray.\n\n\n\nCode\n# catalog search\nsearch2 = catalog.search(\n    collections=['cop-dem-glo-90'],\n    intersects=bbox,\n    datetime=time_range)\nsearch2\n\nitems2 = search2.item_collection()\n\n# number of items \nlen(items2)\n\n\n1\n\n\n\n\nCode\nitem2 = items2[0]\ntype(item2)\n\n\npystac.item.Item\n\n\n\n\nCode\nitem2.assets\n\n\n{'data': &lt;Asset href=https://elevationeuwest.blob.core.windows.net/copernicus-dem/COP90_hh/Copernicus_DSM_COG_30_N34_00_W120_00_DEM.tif?st=2023-11-26T21%3A44%3A42Z&se=2023-12-04T21%3A44%3A42Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2023-11-27T21%3A44%3A41Z&ske=2023-12-04T21%3A44%3A41Z&sks=b&skv=2021-06-08&sig=6ZTP1TCjtgMcILEivkVJshb/9ajT3kuVecLRiFkGBWc%3D&gt;,\n 'tilejson': &lt;Asset href=https://planetarycomputer.microsoft.com/api/data/v1/item/tilejson.json?collection=cop-dem-glo-90&item=Copernicus_DSM_COG_30_N34_00_W120_00_DEM&assets=data&colormap_name=terrain&rescale=-1000%2C4000&format=png&gt;,\n 'rendered_preview': &lt;Asset href=https://planetarycomputer.microsoft.com/api/data/v1/item/preview.png?collection=cop-dem-glo-90&item=Copernicus_DSM_COG_30_N34_00_W120_00_DEM&assets=data&colormap_name=terrain&rescale=-1000%2C4000&format=png&gt;}\n\n\n\n\nCode\n# Open the item‚Äôs data using rioxarray\nsb2 = rioxr.open_rasterio(item2.assets['data'].href)\nsb2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 1200, x: 1200)&gt;\n[1440000 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 -120.0 -120.0 -120.0 ... -119.0 -119.0 -119.0\n  * y            (y) float64 35.0 35.0 35.0 35.0 35.0 ... 34.0 34.0 34.0 34.0\n    spatial_ref  int64 0\nAttributes:\n    AREA_OR_POINT:  Point\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 1200x: 1200...[1440000 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-120.0 -120.0 ... -119.0 -119.0array([-120.      , -119.999167, -119.998333, ..., -119.0025  , -119.001667,\n       -119.000833])y(y)float6435.0 35.0 35.0 ... 34.0 34.0 34.0array([35.      , 34.999167, 34.998333, ..., 34.0025  , 34.001667, 34.000833])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-120.00041666666667 0.0008333333333333334 0.0 35.000416666666666 0.0 -0.0008333333333333334array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([             -120.0, -119.99916666666667, -119.99833333333333,\n                 -119.9975, -119.99666666666667, -119.99583333333334,\n                  -119.995, -119.99416666666667, -119.99333333333334,\n                 -119.9925,\n       ...\n       -119.00833333333334,           -119.0075, -119.00666666666666,\n       -119.00583333333333,            -119.005, -119.00416666666666,\n       -119.00333333333333,           -119.0025, -119.00166666666667,\n       -119.00083333333333],\n      dtype='float64', name='x', length=1200))yPandasIndexPandasIndex(Index([              35.0,  34.99916666666667, 34.998333333333335,\n                  34.9975,  34.99666666666667,  34.99583333333333,\n                   34.995, 34.994166666666665,  34.99333333333333,\n                  34.9925,\n       ...\n        34.00833333333333,            34.0075,  34.00666666666667,\n       34.005833333333335,             34.005,  34.00416666666667,\n        34.00333333333333,            34.0025, 34.001666666666665,\n        34.00083333333333],\n      dtype='float64', name='y', length=1200))Attributes: (3)AREA_OR_POINT :Pointscale_factor :1.0add_offset :0.0\n\n\n\n\nCode\nImage(url=item2.assets['rendered_preview'].href, width=500)\n\n\n\n\n\n\n\nCode\nsb2.sel(band=[1]).plot()\n\n\n&lt;matplotlib.collections.QuadMesh at 0x7e780064bc10&gt;"
  },
  {
    "objectID": "posts/2023-12-14-texas-great-blackout/index.html",
    "href": "posts/2023-12-14-texas-great-blackout/index.html",
    "title": "Analysis of the Impact of Socio-Economic Indicators on Recovery From the 2021 Texas Electric Grid Blackout",
    "section": "",
    "text": "During the period from February 13 to 20, 2021, the southwest U.S. experienced an unusual winter storm, ‚ÄúUri.‚Äù Texas was particularly badly impacted due to a combination of multiple factors. Importantly, Texas‚Äôs electricity is supplied by the intra-state grid operator ERCOT, not connected to the other two major grids that serve the United States and Canada. During the winter storm, ERCOT lost almost half of its generation capacity and didn‚Äôt have the infrastructure to tap into the resources of the other two grids. As a result, major blackouts during February 15-18, 2021 left more than 10 million people without electricity at the peak of the winter storm, with some enduring several days without power. Services dependent on electricity, such as drinking water treatment and medical services, were affected as well. More than 200 people died directly or indirectly as a result of the crisis [2].\nAccording to the report[1] produced by The University of Texas at Austin, the outages were caused by multiple factors, including but not limited to:\n\n\nAll types of generation technologies failed.\n\nDemand forecasts for severe winter storms were too low.\n\nSome power generators were inadequately weatherized.\n\nFailures within the natural gas system.\n\nNatural gas in storage was limited.\n\n\nThe Texas grid didn‚Äôt return to normal operations until Friday, February 19.\n\n\n\nIn this analysis, I explored whether socio-economic factors, specifically household income, influenced the recovery of electricity services in impacted neighborhoods.\nResearch question: Did census tracts within low-income quintiles experience more delays in the recovery of electricity services compared to those in higher income levels?\n\n\n\nThe analysis is based on four datasets.\n\n\nVisible Infrared Imaging Radiometer Suite (VIIRS) data of Houston, TX from 2021-02-07 and 2021-02-16 from NASA‚Äôs Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC) [3]\nNote: Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06, requiring the analysis of two tiles for each day.\nProcessed data is available in VNP46A1 folder.\n\n\nVNP46A1.A2021038.h08v05.001.2021039064328.h5.tif: tile h08v05, collected on 2021-02-07\n\nVNP46A1.A2021038.h08v06.001.2021039064329.h5.tif: tile h08v06, collected on 2021-02-07\n\nVNP46A1.A2021047.h08v05.001.2021048091106.h5.tif: tile h08v05, collected on 2021-02-16\n\nVNP46A1.A2021047.h08v06.001.2021048091105.h5.tif: tile h08v06, collected on 2021-02-16\n\n\n\n\nA Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. Source data: Geofabrik‚Äôs download sites [4]\n\ngis_osm_roads_free_1.gpkg\n\n\n\n\nA Geopackage (.gpkg file) containing only houses in the Houston metropolitan area. Source data: Geofabrik‚Äôs download sites [4]\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\n\nData from the U.S. Census Bureau‚Äôs American Community Survey for census tracts in 2019 [5].\nThe folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS ‚Äúfile geodatabase‚Äù, a multi-file proprietary format that‚Äôs roughly analogous to a GeoPackage file [6].\n\nACS metadata.\n\n\n\n\nI relied on geospatial data and tools to identify locations impacted by the blackouts. Specifically, I used raster and vector data (refer to the Data section) to visualize the correlation between the blackouts and households income.\n\n\n\n\nThe scope of the analysis was limited to Houston metropolitan area.\n\nAnalysis was done only for one day (February 16th 2021) out of 4-5 days of the total blackout period.\n\nAnalysis primarily relied on the satellite data (VIIRS) to identify blackouts.\n\nOnly income was considered in the analysis."
  },
  {
    "objectID": "posts/2023-12-14-texas-great-blackout/index.html#overview",
    "href": "posts/2023-12-14-texas-great-blackout/index.html#overview",
    "title": "Analysis of the Impact of Socio-Economic Indicators on Recovery From the 2021 Texas Electric Grid Blackout",
    "section": "",
    "text": "During the period from February 13 to 20, 2021, the southwest U.S. experienced an unusual winter storm, ‚ÄúUri.‚Äù Texas was particularly badly impacted due to a combination of multiple factors. Importantly, Texas‚Äôs electricity is supplied by the intra-state grid operator ERCOT, not connected to the other two major grids that serve the United States and Canada. During the winter storm, ERCOT lost almost half of its generation capacity and didn‚Äôt have the infrastructure to tap into the resources of the other two grids. As a result, major blackouts during February 15-18, 2021 left more than 10 million people without electricity at the peak of the winter storm, with some enduring several days without power. Services dependent on electricity, such as drinking water treatment and medical services, were affected as well. More than 200 people died directly or indirectly as a result of the crisis [2].\nAccording to the report[1] produced by The University of Texas at Austin, the outages were caused by multiple factors, including but not limited to:\n\n\nAll types of generation technologies failed.\n\nDemand forecasts for severe winter storms were too low.\n\nSome power generators were inadequately weatherized.\n\nFailures within the natural gas system.\n\nNatural gas in storage was limited.\n\n\nThe Texas grid didn‚Äôt return to normal operations until Friday, February 19.\n\n\n\nIn this analysis, I explored whether socio-economic factors, specifically household income, influenced the recovery of electricity services in impacted neighborhoods.\nResearch question: Did census tracts within low-income quintiles experience more delays in the recovery of electricity services compared to those in higher income levels?\n\n\n\nThe analysis is based on four datasets.\n\n\nVisible Infrared Imaging Radiometer Suite (VIIRS) data of Houston, TX from 2021-02-07 and 2021-02-16 from NASA‚Äôs Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC) [3]\nNote: Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Houston lies on the border of tiles h08v05 and h08v06, requiring the analysis of two tiles for each day.\nProcessed data is available in VNP46A1 folder.\n\n\nVNP46A1.A2021038.h08v05.001.2021039064328.h5.tif: tile h08v05, collected on 2021-02-07\n\nVNP46A1.A2021038.h08v06.001.2021039064329.h5.tif: tile h08v06, collected on 2021-02-07\n\nVNP46A1.A2021047.h08v05.001.2021048091106.h5.tif: tile h08v05, collected on 2021-02-16\n\nVNP46A1.A2021047.h08v06.001.2021048091105.h5.tif: tile h08v06, collected on 2021-02-16\n\n\n\n\nA Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. Source data: Geofabrik‚Äôs download sites [4]\n\ngis_osm_roads_free_1.gpkg\n\n\n\n\nA Geopackage (.gpkg file) containing only houses in the Houston metropolitan area. Source data: Geofabrik‚Äôs download sites [4]\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\n\nData from the U.S. Census Bureau‚Äôs American Community Survey for census tracts in 2019 [5].\nThe folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS ‚Äúfile geodatabase‚Äù, a multi-file proprietary format that‚Äôs roughly analogous to a GeoPackage file [6].\n\nACS metadata.\n\n\n\n\nI relied on geospatial data and tools to identify locations impacted by the blackouts. Specifically, I used raster and vector data (refer to the Data section) to visualize the correlation between the blackouts and households income.\n\n\n\n\nThe scope of the analysis was limited to Houston metropolitan area.\n\nAnalysis was done only for one day (February 16th 2021) out of 4-5 days of the total blackout period.\n\nAnalysis primarily relied on the satellite data (VIIRS) to identify blackouts.\n\nOnly income was considered in the analysis."
  },
  {
    "objectID": "posts/2023-12-14-texas-great-blackout/index.html#analysis",
    "href": "posts/2023-12-14-texas-great-blackout/index.html#analysis",
    "title": "Analysis of the Impact of Socio-Economic Indicators on Recovery From the 2021 Texas Electric Grid Blackout",
    "section": "Analysis",
    "text": "Analysis\nThe next few sections provide the steps I applied to complete the analysis. The code is available in the github repository: texas-grid-blackout-2021\n\nSetup\nI used the following libraries throughout the analysis:\n\n\nCode\nknitr::opts_chunk$set(echo = TRUE)\n\n# load libararies\nlibrary(sf)\nlibrary(raster)\nlibrary(stars)\nlibrary(terra)\nlibrary(spData)\nlibrary(spDataLarge)\nlibrary(ggspatial)\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(patchwork)\nlibrary(RColorBrewer)\nlibrary(ggthemes)\n\n\n\n\nIdentifying Blackout Locations\nAs the first step in the analysis, I identified the areas affected by blackouts in the Houston Metropolitan Area on February 16, 2021. To do so, I compared the light intensity data before the storm on February 7, 2021, with the data during the blackout event February 16, 2021, using VIIRS data.\nAssumption: any location that experienced a drop of more than 200 nW cm-2sr-1 experienced a blackout.\nI loaded satellite raster data (files with extention *.tif) using stars package:\n\n\nCode\nnt_20210207_tile5 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif')\nnt_20210207_tile6 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif')\nnt_20210207 &lt;- st_mosaic(nt_20210207_tile5, nt_20210207_tile6) # combine two tiles\n\nnt_20210216_tile5 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif')\nnt_20210216_tile6 &lt;- read_stars('data/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif')\nnt_20210216 &lt;- st_mosaic(nt_20210216_tile5, nt_20210216_tile6) # combine two tiles\n\n\nNext, I created a raster with the difference in light intensity between February 7 and 16, 2021 and reclassified it to the locations that experienced a drop in light intensity of more than 200 nW cm-2sr-1.\n- All locations that experienced a drop of 200 or less nW cm-2sr-1 were assigned NA.\n\n# create a mask with blackout locations\nblkout &lt;- (nt_20210207 - nt_20210216) &gt; 200 \n\n# set locations that didn't experience blackout to NA \nblkout[blkout == FALSE] &lt;- NA \n\nTo proceed, I narrowed down the blackout locations to the Houston metropolitan area by cropping the blackout raster to the area of interest.¬† To do so I converted the raster with the blackout locations into an sf object and constructed a polygon with the boundaries of Houston metropolitan area.\n\n\n\n\n\n\nNote\n\n\n\nTo maintain an accuracy of analysis I applied EPSG: 3083 (NAD83 / Texas Centric Albers Equal Area) projection for the cropped blackout dataset, specifically the dataframe with Houston metropolitan area geometries.\n\n\n\n# convert blackout mask to sf object\nblkout_sf &lt;- blkout %&gt;%\n  st_as_sf() %&gt;%\n  st_make_valid()  # fix invalid geometries\n\n# define coordinates of the Houston metropolitan area\nhstn_coords &lt;- rbind(c(-96.5, 29), \n                     c(-96.5, 30.5), \n                     c(-94.5, 30.5), \n                     c(-94.5, 29),\n                     c(-96.5, 29))  \n\n# create a polygon of the Houston area \nhstn &lt;- st_polygon(list(hstn_coords)) %&gt;% \n  st_sfc(crs = \"EPSG:4326\")\n\n# crop the blackout mask to the Houston area\nhstn_blkout &lt;- blkout_sf[hstn, ] %&gt;% \n  st_transform(crs = \"EPSG:3083\") \n\n\n\n\n\n\n\n\nExluding roads from the area of interest\nHighways in the United States are commonly lit during the night, contributing in a large proportion to the lights visible from space. However, the highways and the area adjustent to them don‚Äôt have any households, and therefore can distort the analysis results by mistakenly identifying areas with reduced traffic as areas without power. To minimize the error, I excluded any areas within 200-meter radius of all highways from the analysis.\nI used the following sql query to load the roads dataset from gis_osm_roads_free_1.gpkg:\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n\nCode\n# read data from SQL\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass = 'motorway'\"\n\nhighways &lt;- st_read(\"data/gis_osm_roads_free_1.gpkg\", \n                    query = query,\n                    quiet = TRUE) %&gt;% \n  select('geom') %&gt;% # keep only geometries\n  st_transform(crs = st_crs(hstn_blkout)) # reproject highways dataframe to the crs of Houston blackout dataframe\n\n# verify the CRS are the same\nprint(paste0('CRS of of the highways and Houston blackout dataframes match: ', \n             crs_match &lt;- st_crs(highways) == st_crs(hstn_blkout)))\n\n\nIn the following step, I determined the areas within 200-meter radius and excluded these areas from the main Houston blackout dataset using spatial subsetting.\n\n# create a buffer of 200 m around highways\nhwy_buffer &lt;- st_buffer(highways, dist = 200) %&gt;% \n  st_union() # dissolve buffers\n\n# find areas that experienced blackouts and are further than 200m from a highway\nhstn_blkout_no_hwy &lt;- st_difference(hstn_blkout, hwy_buffer)\n\n\n\n\n\n\n\n\nIndentifying homes impacted by blackouts\nMoving on, I identified the population of homes affected by the blackouts to explore the extent to which socio-economic factors played a role in the recovery.\nI used the following sql query to load the roads dataset from gis_osm_buildings_a_free_1.gpkg:\nSELECT *  FROM gis_osm_buildings_a_free_1\nWHERE (type IS NULL AND name IS NULL)\nOR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\n\n\nCode\n# read data from SQL\nquery_bldg &lt;- (\"SELECT * FROM gis_osm_buildings_a_free_1\n               WHERE (type IS NULL AND name IS NULL)\n               OR type IN ('residential', 'apartments', 'house', 'static_caravan', 'detached')\")\n\nbldg &lt;- st_read(\"data/gis_osm_buildings_a_free_1.gpkg\", \n                query = query_bldg,\n                quiet = TRUE) %&gt;% \n  st_transform(crs = \"EPSG:3083\")\n\n# verify the CRS are the same\nprint(paste0('CRS of of the builduings and Houston blackout dataframes match: ', \n             crs_match &lt;- st_crs(bldg) == st_crs(hstn_blkout_no_hwy)))\n\n\nTo obtain the population of homes affected by the blackouts in the Houston metropolitan area, I subsetted the buildings dataset using the resulting dataframe from the prior step (an sf dataframe indicating blackouts in the Houston metropolitan area beyond a 200-meter radius from highways).\n\n# filter homes within blackout areas\nbldg_blkout &lt;- bldg[hstn_blkout_no_hwy, ] \n\n\n\nAnalyzing the impact of socioeconomic factors on recovery\nFinally, I analyzed whether households income played a role on the recovery from the power outage. As a socio-economic indicator I used the income data from the 2019 US Census Tract.\nI loaded data using st_read function from the sf library:\n\n\nCode\n# read in geometries data\nct_geom &lt;- st_read('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb', \n                   layer = 'ACS_2019_5YR_TRACT_48_TEXAS',\n                   quiet = TRUE) %&gt;% \n  st_transform(crs = 'EPSG:3083') %&gt;% \n  filter(COUNTYFP %in% c(339, 291, 167, 015, 039, 071, 157, 201, 473)) # Filter Houston counties\n\n# read in income data \nct_income &lt;- st_read('data/ACS_2019_5YR_TRACT_48_TEXAS.gdb', \n                     layer = 'X19_INCOME',\n                     quiet = TRUE) %&gt;% \n  select(GEOID,median_income = B19013e1) \n\n\nIn the following step, I joined the income data on the impacted homes dataset to determine the total number of homes impacted by the blackouts.\n\n#join the income data to the census tract geometries\nct_income &lt;- dplyr::left_join(ct_geom, ct_income,\n                              by = c(\"GEOID_Data\" = \"GEOID\")) %&gt;%\n  st_transform(crs = \"EPSG:3083\")\n\n# spatially join census tract data with buildings impacted by blackouts\nhstn_ct_blkout &lt;- st_join(ct_income, \n                          bldg_blkout, \n                          left = TRUE) %&gt;% \n  group_by(NAME) %&gt;% \n  summarise(blackout = any(!is.na(osm_id)), \n         median_income = first(median_income))\n\n\n\nCode\n# get the count of census tracts impacted by blackouts\nct_blkout_tot_impacted &lt;- hstn_ct_blkout %&gt;% \n  filter(blackout == TRUE) %&gt;% \n  summarise(total = n())\n\ncat(\"Total census tracts in Houston area impacted by the blackouts:\",\n    ct_blkout_tot_impacted$total)\n\n\nTotal census tracts in Houston area impacted by the blackouts: 717\n\n\n\n\nResults\nI‚Äôve visualized the analysis results using the following representations:\n\na map illustrating the distribution of median income across census tracts, highlighting areas affected by blackouts.\n\n\n\nCode\n# define income quintiles\nquintiles &lt;- quantile(hstn_ct_blkout$median_income, \n                      probs = seq(0, 1, by = 0.2), na.rm = TRUE)\nquintiles_labs &lt;- c('20%', '40%' ,'60%', '80%','100%')\n\n\n# create a new variable with quintile labels\nhstn_ct_blkout$income_quintile &lt;- cut(hstn_ct_blkout$median_income, \n                          breaks = quintiles, \n                          labels = quintiles_labs, \n                          include.lowest = TRUE)\n\ncolor_palette &lt;- brewer.pal(length(quintiles_labs), \"YlGn\")\n\n# create centroids for census tract impacted by blackout\nct_centroids &lt;- hstn_ct_blkout %&gt;% \n  filter(blackout == TRUE) %&gt;% \n  st_centroid()\n\n# plot a map\nm1 &lt;- ggplot() +\n  annotation_map_tile(zoom = 10, cachedir = system.file(\"rosm.cache\", package = \"ggspatial\")) +\n  geom_sf(data = hstn_ct_blkout, aes(fill = income_quintile), alpha = 0.8) +\n  scale_fill_manual(values = setNames(color_palette, quintiles_labs),\n                    name = 'Income Quintiles') +\n  geom_sf(data = ct_centroids, size = 0.5, color = '#000080', alpha = 0.8) +\n  labs(\n    title = 'Impact of February 2021 Blackout in Houston, TX',\n    subtitle = 'Median Income Distribution by Census Tract') +\n  theme_map() +\n  theme(legend.position = \"right\")\nm1\n\n\n\n\n\n\na histogram that compares impacted vs non-impacted census tracts by the income level:\n\n\n\nCode\n# Plot histogram\nhist &lt;- hstn_ct_blkout %&gt;% \n  ggplot(aes(x = median_income, fill = blackout)) +\n  xlab('Median Income') +\n  geom_histogram(binwidth = 1000) +\n  labs(\n    title = 'Income Distribution by Census Tract',\n    subtitle = 'Comparison between Census Tracts Impacted and Not Impacted by Blackouts',\n    fill = 'Impacted by Blackouts'\n  ) +\n  theme_minimal()\n\nhist\n\n\n\n\n\n\nFindings\nAccording to the analysis results, households across all levels of income distribution were affected by the blackouts. Those in the lowest income distribution were more likely to experience the impact than not. This is unsurprising, considering that the outage was caused by a rare winter storm event for which grid operators were not prepared. Any household connected to the grid was affected by the blackouts.\nAdditional analysis is required to understand whether lower income households were disproportionately impacted by the outage. Specifically, we need to examine the speed at which grid services were restored in the impacted census tracts and whether socioeconomic factors played a role.\nThe primary limitation of the analysis is the lack of data on household activities for the duration of blackouts. Additionally, it does not incorporate other socioeconomic factors, such as education and race.\n\n\n\nNext steps\n\nExtend the analysis to the rest of the period of the blackouts: February 15-19, 2021.\n\nInclude blackouts data from utilities to make the analysis more robust.\n\nInclude other socio-economic indicators: unemployment rate, race, education level.\n\n\nAttribution\nThe analysis was completed as part of the [Geospatial Analysis & Remote] Sensing(https://ryoliver.github.io/EDS_223_spatial_analysis/) coursework.\n\nThe data was prepared by Dr.¬†Ruth Oliver, Bren School of Environmental Science & Management, University of California, Santa Barbara\n\n\nCitations\n[1] UTAustin (2021) EventsFebruary2021TexasBlackout 20210714.Pdf. Accessed 15 Dec.¬†2023.\n[2] Svitek, Patrick. ‚ÄúTexas Puts Final Estimate of Winter Storm Death Toll at 246.‚Äù The Texas Tribune, 2 Jan.¬†2022, www.texastribune.org/2022/01/02/texas-winter-storm-final-death-toll-246. Accessed 13 Nov.¬†2023.\n[3] Level-1 and Atmosphere Archive & Distribution System Distributed Active Archive Center - LAADS DAAC. https://ladsweb.modaps.eosdis.nasa.gov/. Accessed 15 Dec.¬†2023.\n[4] Geofabrik Download Server. https://download.geofabrik.de/. Accessed 15 Dec.¬†2023.\n[5] https://www.census.gov/programs-surveys/acs\n[6] https://desktop.arcgis.com/en/arcmap/latest/manage-data/administer-file-gdbs/file-geodatabases.htm"
  },
  {
    "objectID": "posts/2023-12-08-dunkelflaute/index.html",
    "href": "posts/2023-12-08-dunkelflaute/index.html",
    "title": "Powering Through the Dark: Dunkelflaute and Grid Resilience",
    "section": "",
    "text": "In 2015 the International Energy Agency (IEA) welcomed the landmark Paris Agreement with the quote, ‚ÄúThe Paris Agreement is nothing less than a historic milestone for the global energy sector. It will speed up the transformation of the energy sector by accelerating investments in cleaner technologies and energy efficiency.‚Äù The IEA should know: the organization estimates that the energy sector is the single largest emitter of greenhouse gases (GHG), accounting for three quarters of all GHG emissions globally. Meeting climate targets requires that 70 % of global electricity generation come from wind and solar combined according to the IEA 2050 zero emissions scenario. Fortunately, we have seen unprecedented growth in both solar and wind energy in recent years, reaching 12% of global electricity generation in 2022. But the sun does not always shine and the wind does not always blow. Some worry that overreliance on renewable energy sources, which are intermittent in nature, will cause grid reliability problems.\nDunkelflaute, a German word that translates to ‚Äúdark doldrums‚Äù, describes a period of simultaneous reduction in wind and solar power generation. This period typically occurs during the winter months when there is less sunlight and low wind. Renewable energy sources do not produce sufficient electricity during a Dunkelflaute, and this poses challenges to balance the grid and effectively respond to energy demands. Though the occurrence of these conditions is rare, usually five consecutive days once a year, the damage can be significant. In February 2021, the Texas grid experienced a major power outage during a winter storm. Power plants across the state failed to operate in the extreme cold temperatures, leaving more than 10 million people without electricity at the peak of the winter storm, with some enduring several days without power. Services dependent on electricity such as drinking water treatment and medical services were affected as well. More than 200 people died directly or indirectly as a result of the crisis. Even though this event was not caused by the intermittent nature of renewable energy generation, it provides an example of what could happen when a grid is not able to meet energy demand when it is needed the most. Still, there are rays of hope in potential solutions.\nNot all gloom Researchers from the Delft University of Technology identified possible solutions to mitigate Dunkelflaute‚Äôs effect on energy demand supply. The research suggests that meteorological conditions are not necessarily the same across all European countries. This allows countries to optimize energy supply lines with each other. For instance, when there is low solar radiation and still weather in Northern Germany, the South of France might experience sunny and windy conditions. Tight coordination between the two countries would allow France to divert excess energy from its grid to Germany.\n\n\n\n\n\nOther methods to mitigate the effects of low energy production due to prolonged unfavorable meteorological conditions are available or under investigation. Utility-scale energy storage, such as lithium-ion battery installations and salt caverns, can help in balancing the grid during periods of high demand and low energy production. Salt caverns, large underground salt deposits, can be used as hydrogen holding tanks according to the researchers at the University of Texas at Austin‚Äôs Bureau of Economic Geology. These energy banks can be used to store energy during the periods of low demand and release it during high demand when energy production is insufficient. Direct-current-type ultra-high-voltage (UHVDC) transmission lines for global energy transition and climate change is another solution that promises to distribute renewable energy to where it is needed the most.\nWhile the transition to renewable energy sources poses its challenges, the benefits are undeniable: cleaner air, reduced levels of pollution, decrease in GHG emissions, and more equitable energy distribution warrant more innovation to ensure grid reliability. Advancements in energy storage, demand management, and meteorological forecasting will enable grid operators to transition to green energy sources. However, it is crucial to recognize that urgent action is needed to ensure that utilities are well-prepared for this transition without compromising the wellbeing of their customers.\n\n\n\nCitationBibTeX citation:@online{protsukha2023,\n  author = {Protsukha, Oksana},\n  title = {Powering {Through} the {Dark:} {Dunkelflaute} and {Grid}\n    {Resilience}},\n  date = {2023-12-08},\n  url = {https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nProtsukha, Oksana. 2023. ‚ÄúPowering Through the Dark: Dunkelflaute\nand Grid Resilience.‚Äù December 8, 2023. https://oksanaprotsukha.github.io/portfolio/projects/2023-11-06-my-first-post.html."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oksana Protsukha",
    "section": "",
    "text": "Oksana is deeply passionate about environmental data science and its critical role in the decarbonization and energy sectors. She holds an MBA from San Francisco State University and is currently pursuing a Master‚Äôs degree at Bren School. With eight years of experience as a Business Analyst and Technical Product Manager at Tesla Inc., she is now actively pursuing a career pivot into environmental data science. Her goal is to assist companies in addressing data-related challenges within the energy and decarbonization sectors.\n\n\nMaster of Environmental Data Science | University of California, Santa Barbara  Master of Business Administration | California State University, San Francisco"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Oksana Protsukha",
    "section": "",
    "text": "Master of Environmental Data Science | University of California, Santa Barbara  Master of Business Administration | California State University, San Francisco"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Is EV Charging Infrastructure Equitable?\n\n\n\nDataViz\n\n\nR\n\n\nEnergy\n\n\nEV\n\n\n\nInfographic that addresses equity within EV charging network distribution in the United States\n\n\n\nOksana Protsukha\n\n\nMar 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBias in Data Science\n\n\n\nData Science\n\n\nEnvironmental Justice\n\n\n\nExplore biases woven into the data science life cycle and how they impact outcomes.\n\n\n\nOksana Protsukha\n\n\nDec 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of Socio-Economic Indicators on EV Charging Infrastructure\n\n\n\nStatistics\n\n\nR\n\n\nEnergy\n\n\nEV\n\n\n\nDo inequality trends persist in EV charging network infrastructure?\n\n\n\nOksana Protsukha\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of the Impact of Socio-Economic Indicators on Recovery From the 2021 Texas Electric Grid Blackout\n\n\n\nGeoSpatial\n\n\nEnergy\n\n\nEnvironmental Justice\n\n\nR\n\n\n\nIn this blog post, we investigate whether low-income census tracts faced more delays in electricity service recovery than higher-income areas.\n\n\n\nOksana Protsukha\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpact of the Thomas Fire on AQI in Santa Barbara County, CA (2017)\n\n\n\nGeoSpatial\n\n\nPython\n\n\nWild Fires\n\n\nAir Quality\n\n\n\nData Analysis of Thomas Fire‚Äôs Impact on Air Quality Index (AQI) in Santa Barbara County (2017)\n\n\n\nOksana Protsukha\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPowering Through the Dark: Dunkelflaute and Grid Resilience\n\n\n\nEnergy\n\n\nRenewables\n\n\nEnvironmental writing\n\n\n\n\n\n\n\nOksana Protsukha\n\n\nDec 8, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-12-15-se-ev-infrastructure/index.html",
    "href": "posts/2023-12-15-se-ev-infrastructure/index.html",
    "title": "Impact of Socio-Economic Indicators on EV Charging Infrastructure",
    "section": "",
    "text": "In this analysis I explore inequality trends in electric vehicle (EV) charging network distribution in California. An earlier study from 2020 on this topic identified disparities across race and income in EV charging infrastructure [1]. However, we have seen a 136 % increase in new zero-emissions vehicle (ZEV) sales and 33 % increase in charging stations deployment in California since the study was published [2]. In my analysis I aim to identify whether the earlier trends of inequality in EV infrastructure deployment persist.\n\n\n\nAre electric vehicle charging stations less likely to be installed in low-income communities in comparison to higher-income ones?\n\n\n\n\nLack of EV charging infrastructure is one of the primary barriers to EV adoption among the population. The Advanced Clean Cars II regulations mandate that all new vehicle sales in California should be ZEV by 2035 [3]. Equitable distribution of EV charging infrastructure is critical to prevent low income and disadvantaged communities from being left behind during the transition to a green economy in California.\n\n\n\nI used the following two datasets in the analysis.\n\n\nNational Renewable Energy Laboratory (NREL) maintains a comprehensive database of EV charging stations across the United States and Canada. It encompasses over 70 different attributes. Data is available to download via API [4].\n\n\n\nEJScreen stands for Environmental Justice Screening and Mapping Tool [5]. According to the developer‚Äôs documentation ‚ÄúEJSCREEN is an environmental justice mapping and screening tool that provides the U.S. Environmental Protection Agency (EPA) with a nationally consistent dataset and approach for combining environmental and demographic indicators.‚Äù [6] It includes 12 environmental indicators, 6 demographic indicators and 12 EJ indexes. The scope of the analysis was limited to the the following two socio-economic indicators:\n- proximity traffic\n- low income indicators in the\n\nAdditional details and metadata are available on EPA website.\n\n\n\n\n\n\nThe scope of the analysis is limited to the state of California. The analysis did not include the differentiation between urban vs rural areas and didn‚Äôt account for EV ownership.\n\n\n\n\ncensus blocks with the ratio of 0.05% of level 2 or DC charging ports were identified as having EV charging available. This assumes there is at least 1 level 2 or DC charging port per 2000 people.\nlevel 1 charging ports alone were considered insufficient for public charging infrastructure to incentivize EV ownership.\n\n\n\n\n\nEV: electrical vehicle\n\nEV charging station: equipment that connects an electric vehicle (EV) to a source of electricity to recharge electric cars, neighborhood electric vehicles and plug-in hybrids.\n\nCharging port: the system within a charger that charges one EV. A charging port may have multiple connectors, but it can provide power to charge only one EV through one connector at a time. One charging station can have multiple ports.\n\n\n\n\n\n\nSet up hypotheses: NULL and Alternative.\n\nPerform data exploration to identify an optimal dependent variable.\n\nDefine statistical model\n\n\nApply linear model with multi-beta parameters and check the model fit.\nApply logistical (binary) model with multi-beta parameters if linear model fit is not suitable.\n\n\n\n\n\n\n\nParameters such as EV ownership, approvals process, grid infrastructure were not included in the analysis.\nOnly income was used as a socio-economic indicator."
  },
  {
    "objectID": "posts/2023-12-15-se-ev-infrastructure/index.html#overview",
    "href": "posts/2023-12-15-se-ev-infrastructure/index.html#overview",
    "title": "Impact of Socio-Economic Indicators on EV Charging Infrastructure",
    "section": "",
    "text": "In this analysis I explore inequality trends in electric vehicle (EV) charging network distribution in California. An earlier study from 2020 on this topic identified disparities across race and income in EV charging infrastructure [1]. However, we have seen a 136 % increase in new zero-emissions vehicle (ZEV) sales and 33 % increase in charging stations deployment in California since the study was published [2]. In my analysis I aim to identify whether the earlier trends of inequality in EV infrastructure deployment persist.\n\n\n\nAre electric vehicle charging stations less likely to be installed in low-income communities in comparison to higher-income ones?\n\n\n\n\nLack of EV charging infrastructure is one of the primary barriers to EV adoption among the population. The Advanced Clean Cars II regulations mandate that all new vehicle sales in California should be ZEV by 2035 [3]. Equitable distribution of EV charging infrastructure is critical to prevent low income and disadvantaged communities from being left behind during the transition to a green economy in California.\n\n\n\nI used the following two datasets in the analysis.\n\n\nNational Renewable Energy Laboratory (NREL) maintains a comprehensive database of EV charging stations across the United States and Canada. It encompasses over 70 different attributes. Data is available to download via API [4].\n\n\n\nEJScreen stands for Environmental Justice Screening and Mapping Tool [5]. According to the developer‚Äôs documentation ‚ÄúEJSCREEN is an environmental justice mapping and screening tool that provides the U.S. Environmental Protection Agency (EPA) with a nationally consistent dataset and approach for combining environmental and demographic indicators.‚Äù [6] It includes 12 environmental indicators, 6 demographic indicators and 12 EJ indexes. The scope of the analysis was limited to the the following two socio-economic indicators:\n- proximity traffic\n- low income indicators in the\n\nAdditional details and metadata are available on EPA website.\n\n\n\n\n\n\nThe scope of the analysis is limited to the state of California. The analysis did not include the differentiation between urban vs rural areas and didn‚Äôt account for EV ownership.\n\n\n\n\ncensus blocks with the ratio of 0.05% of level 2 or DC charging ports were identified as having EV charging available. This assumes there is at least 1 level 2 or DC charging port per 2000 people.\nlevel 1 charging ports alone were considered insufficient for public charging infrastructure to incentivize EV ownership.\n\n\n\n\n\nEV: electrical vehicle\n\nEV charging station: equipment that connects an electric vehicle (EV) to a source of electricity to recharge electric cars, neighborhood electric vehicles and plug-in hybrids.\n\nCharging port: the system within a charger that charges one EV. A charging port may have multiple connectors, but it can provide power to charge only one EV through one connector at a time. One charging station can have multiple ports.\n\n\n\n\n\n\nSet up hypotheses: NULL and Alternative.\n\nPerform data exploration to identify an optimal dependent variable.\n\nDefine statistical model\n\n\nApply linear model with multi-beta parameters and check the model fit.\nApply logistical (binary) model with multi-beta parameters if linear model fit is not suitable.\n\n\n\n\n\n\n\nParameters such as EV ownership, approvals process, grid infrastructure were not included in the analysis.\nOnly income was used as a socio-economic indicator."
  },
  {
    "objectID": "posts/2023-12-15-se-ev-infrastructure/index.html#analysis",
    "href": "posts/2023-12-15-se-ev-infrastructure/index.html#analysis",
    "title": "Impact of Socio-Economic Indicators on EV Charging Infrastructure",
    "section": "Analysis",
    "text": "Analysis\n\nHypothesis\nThe analysis is based on the following hypotheses:\nH0 (Null Hypothesis): Income level does not have an impact on EV charging infrastructure within a census block area, even when controlling for proximity to traffic.\nHA (Alternative Hypothesis): The probability that EV charging infrastructure is available within a census block area with a higher level of low-income households is lower, even when controlling for proximity to traffic.\n\n\nSetup\nI used the following libraries throughout the analysis:\n\n\nCode\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(readr)\nlibrary(gt)\nlibrary(openintro)\nlibrary(ggplot2)\nlibrary(modelr)\nlibrary(lterdatasampler)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(maptiles)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(patchwork)\n\n\n\n\nData wrangling and exploration\nTo start, I downloaded the two datasets from NREL and EPA EJScreen (refer to the data section). Since both datasets had spacial attributes and no other attributes in common, I used spatial joins and subsetting to combine them into a single dataset. Following this, I converted the final dataset into a dataframe by eliminating geometries. This allows to enhance performance of the subsequent steps by reducing the number of datapoints to process.\n\nEV charging data import and wrangling:\n\n\nCode\n# Get charging stations for California\n# Specify API endpoint and parameters\napi_url &lt;- \"https://developer.nrel.gov/api/alt-fuel-stations/v1\"\n\napi_params &lt;- list(\n  format = \"json\",  \n  api_key = api_key, \n  status = \"E\",  # E = charging stations in operation\n  access = \"public\",\n  fuel_type = \"ELEC\",\n  cng_vehicle_class = \"LD\", # LD = light duty vehicles\n  country = \"US\",\n  state = \"CA\",\n  limit = \"all\"\n)\n\n# Construct the full URL with parameters\nrequest_url &lt;- modify_url(api_url, query = api_params)\n\n# Make a GET request to the API\nresponse &lt;- GET(request_url)\n\n# Extract the fuel_stations data from the response\nfuel_stations_df &lt;- content(response, \"parsed\")$fuel_stations\n\n# Convert the list to a dataframe\nfuel_stations_df &lt;- as.data.frame(do.call(rbind, fuel_stations_df)) \n\n# Select the fields for analysis\nfuel_stations_df_clean &lt;- fuel_stations_df %&gt;% \n  filter(!is.na(fuel_type_code)) %&gt;% \n  select('id', 'access_code','owner_type_code', 'open_date', 'restricted_access', 'maximum_vehicle_class','facility_type', 'city','state','zip','ev_workplace_charging', 'ev_level1_evse_num', 'ev_level2_evse_num', 'ev_dc_fast_num' , 'longitude', 'latitude')\n\n# Convert data frame to sf object\nfuel_stations_sf &lt;- fuel_stations_df_clean %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# identify which points are outside the boundarie of California\ncalifornia_bbox &lt;- st_bbox(c(xmin = -124.409591, ymin = 32.534156, \n                             xmax = -114.131211, ymax = 42.009518), \n                           crs = st_crs(fuel_stations_sf))\n\nca_ev_sf &lt;- fuel_stations_sf %&gt;% \n  st_set_crs(st_crs(california_bbox)) %&gt;%  # Set CRS to the same as your data\n  st_crop(california_bbox)  # Crop to the bounding box of California\n\n\n\n\nEJScreen data import and wrangling:\n\n\nCode\n# read in geodatabase of EJScreen data at the Census Block Group level\nejscreen &lt;- st_read(file.path(rootdir,'/data/EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb/'))\nejscreen_meta &lt;- st_read(file.path(rootdir, \"data/EJSCREEN_2023_BG_Columns.xlsx\"))\n\n# transform ejscreen into sf object\nejscreen_sf &lt;- st_as_sf(ejscreen)\n\n# clean ejscreen\nejscreen_sf_clean &lt;- ejscreen_sf %&gt;% \n  select('ID', 'STATE_NAME', 'ST_ABBREV', 'CNTY_NAME', 'PTRAF', 'P_PTRAF', 'P_LOWINCPCT', 'LOWINCOME','LOWINCPCT', 'ACSTOTPOP', 'P_UNEMPPCT', 'P_PEOPCOLORPCT', 'P_UNEMPPCT', 'P_PNPL', 'P_PRMP', 'P_PTSDF') %&gt;% \n  rename(census_block_id = 'ID') %&gt;% \n  clean_names()\n\n# filter EJ to California state \nejscreen_ca_sf &lt;- ejscreen_sf_clean %&gt;% \n  filter(state_name == 'California')\n\n\nBefore proceeding to the modeling steps, I visualized the data to ensure that we are working with the accurate datasets.\n\n\n\n\n\n\n\n\n\n\n\nStatistical Modeling\nIn order to proceed with the modeling, I added a new binary variable to the dataset to define whether a given census block had or didn‚Äôt have EV charging available:\n\n# Set census blocks that have at least 0.05% of level 2 or DS ports per capita to 1, else to 0\nports_per_capita_clean_logit &lt;- ports_per_capita_excl_level1_clean %&gt;%\n  mutate(chargers_available_per_capita = case_when(\n    total_ports_capita_pct &gt; 0.05 ~ 1,\n    total_ports_per_block &lt;= 0.05 ~ 0\n  )) %&gt;% \n  select(-id,-total_ports) %&gt;% \n  na.omit\n\n\n\n\nEV Charging Availability per Census Block, California\n\n\n\n\n\n\n\n\n\n\nCensus Block ID\nTotal Ports per Block\nTotal Ports Capita Pct\nP_ptraf\nP_lowincpct\nEVSE Available\n\n\n\n\n060014001001\n1\n0.0509424\n27\n26\n1\n\n\n060014001002\n0\n0.0000000\n88\n17\n0\n\n\n060014002001\n0\n0.0000000\n91\n8\n0\n\n\n060014002002\n0\n0.0000000\n96\n27\n0\n\n\n060014003001\n0\n0.0000000\n93\n6\n0\n\n\n060014003002\n2\n0.1232286\n85\n17\n1\n\n\n\n\n\n\nBasic data visualization\nTo visualize the distribution of EV infrastructure across the percentiles for low income and proximity to traffic, I used histogram:\n\n\n\n\n\nFrom the graphs, we observe a marginal decrease in available EV infrastructure as the percentile of low-income households within a census block increases. Additionally, there is a substantial increase in the availability of EV charging infrastructure with closer proximity to traffic.\n\nLet‚Äôs run logarithmic regression to confirm whether out intuition is correct.\n\nmod_ev_multi = glm(chargers_available_per_capita~p_lowincpct+p_ptraf, \n                   data = ports_per_capita_clean_logit, family = 'binomial')\n\ngg = ggplot(data = ports_per_capita_clean_logit, aes(x = p_lowincpct, y = chargers_available_per_capita)) +\n  geom_jitter(width = 0, height = 0.05, alpha = 0.8, size = 0.1) +\n  geom_smooth(method = 'lm', se = FALSE) +\n  theme_minimal()\n\nlog_g &lt;- gg + \n  geom_smooth(method = 'glm', se = FALSE, color = 'red', \n              method.args = list(family = 'binomial')) +\n  labs(x = \"Low-Income Percentile\", y = \"EV Chargering Available\")\n\n\n\nInterpretation:\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nCoefficients\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.1166684\n0.0295272\n-37.81830\n0\n\n\np_lowincpct\n-0.0052204\n0.0003880\n-13.45622\n0\n\n\np_ptraf\n0.0186964\n0.0003952\n47.31408\n0\n\n\n\n\n\nCoefficients:\n\nIntercept: according to the model, when both low income percent percentile and proximity to traffic percentile are zero, the estimated log-odds of EV infrastructure availability in a given census block is -1.15683. This intuitively makes sense, since we typically would not expect the need of EV charging (or it would be very low) if a census block is remote.\n\np_lowincpct: this indicates that with 1 percentile increase in low-income percent percentile in a census block controlling for proximity to traffic percentile, we estimate -0.00549 change in log-odds of EV charging availability. Again, this makes sense and alignes with our Alternative Hypothesis. In general, we would expect few EV charging stations in low-income communities.\n\np_ptraf: this indicates that with 1 percentile increase in proximity to traffic percentile in a census block controlling for low-income percent percentile, we estimate 0.01919 change in log-odds of EV charging availability. This alignes with our experience and intuition, since we usually expect more EV charging stations in close proximity to traffic.\np-value: p-value is are well below 0.05 significance level, indicating high statistical significance of all the coefficients in predicting the availability of EV charging infrastructure. Therefore, we can reject the Null Hypothesis and conclude that there is a relationship between a low-income and ev charging infrastructure availability even when controlling for proximity to traffic.\n\n\n\n\nFindings\nAccording to the regression results, we can conclude that the probability of having EV charging infrastructure in the census blocks with more low-income households is lower in comparison to higher income census blocks. However, the values of the coefficients are low and from the graphs we observe that there is a rather small gap in EV infrastructure availability based on income distribution.\nAdditional analysis is required to confirm the findings by including other variables in the model, such as race, unemployment, EV ownership ratio.\n\n\nAttributions\nThe analysis was completed as part of the Statistics for Environmental Data Science coursework.\n\n\nCitations\n[1] Hsu, Chih-Wei, and Kevin Fingerman. ‚ÄúPublic Electric Vehicle Charger Access Disparities across Race and Income in California.‚Äù Transport Policy 100 (January 1, 2021): 59‚Äì67. https://doi.org/10.1016/j.tranpol.2020.10.003.\n\n[2] Commission, California Energy. ‚ÄúZEV and Infrastructure Stats Data.‚Äù California Energy Commission. California Energy Commission, current-date. https://www.energy.ca.gov/files/zev-and-infrastructure-stats-data.\n\n[3] ‚ÄúAdvanced Clean Cars II | California Air Resources Board.‚Äù Accessed November 10, 2023.https://ww2.arb.ca.gov/our-work/programs/advanced-clean-cars-program/advanced-clean-cars-ii.\n\n[4] All Stations API | NREL: Developer Network. https://developer.nrel.gov/docs/transportation/alt-fuel-stations-v1/all/. Accessed 15 Dec.¬†2023.\n\n[5] US EPA, OEJECR. Purposes and Uses of EJScreen. 20 Oct.¬†2014, https://www.epa.gov/ejscreen/purposes-and-uses-ejscreen.\n\n[6] EJSCREEN: Environmental Justice Screening and Mapping Tool | U.S. Climate Resilience Toolkit. https://toolkit.climate.gov/tool/ejscreen-environmental-justice-screening-and-mapping-tool#. Accessed 15 Dec.¬†2023."
  },
  {
    "objectID": "posts/2023-12-27-bias-in-data-science/index.html",
    "href": "posts/2023-12-27-bias-in-data-science/index.html",
    "title": "Bias in Data Science",
    "section": "",
    "text": "Bias in Data Science\n\nWhat is Data Science?\nThese days we hear about data science all the time; it is all around us, embedded in the latest AI news about ChatGPT, as well as in mundane posts on social media. But what is it exactly? There are many opinions out there, with attempts to capture the essence of data science. My favorite one is from Cassie Kozyrkov, Chief Decision Scientist: ‚ÄúData science is the discipline of making data useful.‚Äù Now, what does it mean to be ‚Äúuseful‚Äù? I am going to define it as guiding decisions to create a more equitable and comfortable world to live in. You can choose your own definitions, but for the purposes of this post we will use this one. So, how do we go about making the world a better place with data? I propose to start with the data science life cycle framework, which I developed to aid in identifying inherent biases at each phase of a data analysis project. This tool is designed to raise awareness of the various biases in data science, and promote a more responsible approach of working with data and making it useful.\n\n\nData science life cycle\n\n\n\n\n\nData science life cycle is the journey to uncover patterns in the world around us to solve real-life problems with the help of data and data science tools. We can define the following steps of this journey: problem definition, data collection, data processing, data analysis, peer review, data communication and data consumption. While not all steps apply to every data analysis project or study, these capture the essence of the process. Next, let‚Äôs review each step in more detail and discuss any obvious and not so obvious pitfalls.\n\n\nData science life cycle phases\n\nProblem definition.\nDuring this step, a data professional or researcher states a research question, defines a problem statement, establishes objectives, and defines the success criteria. The problem statement guides data collection and influences all subsequent steps. It‚Äôs important to understand that two different people might have very different convictions about the same subject or research goals. This is known as a researcher bias [1].\nScenario: For instance, we are trying to understand whether fossil fuels are directly related to the warming effect of the Earth. A researcher working at a fossil fuel company might be interested in identifying the patterns that would show that fossil fuels have lower impact than usually thought. At the same time, an analyst at a renewable energy company, tasked with advocating for replacing fossil fuels, may focus on uncovering patterns that reveal the direct link between fossil fuel consumption and greenhouse gas emissions, while downplaying potential negative impacts on employment during the transition.\n\n\n\nData collection.\nThis is a critical step in the research as it directly impacts the quality of data that data professionals will be working with in the future. Data can be collected using multiple different methods, which can be divided into active and passive categories. Active data collection methods involve direct interaction with the subjects of research via surveys, field studies, literature review, interviews and others. Passive methods, on the other hand, rely on existing technologies to sample data, such as remote sensing. While passive data collection naturally offers more objectivity, it is not entirely free of bias. In the end, it‚Äôs humans who program the machines and either due to complexity, cost or simple typos might introduce errors in the collection process, compromising data quality. Automation bias - the tendency to depend excessively on automated systems, which can lead to mistaken automated information overriding correct decisions - is real and it‚Äôs important to be aware of it. Active methods, in their turn, are prone to multiple biases such as sampling bias, confirmation bias, interviewer bias, overgeneralization bias and availability bias among others.\nScenario: A group of domestic and international researchers is studying climate adaptation across communities in Ghana. In Ghana most of the infrastructure is concentrated in the South of the country, where the weather conditions are milder and getting to the North of the country is expensive and time consuming [2]. Therefore researchers decide to focus on sampling the data from the Southern regions and if required extrapolate the findings to the rest of the country. In this scenario researchers committed availability, selection and interviewer biases since they collected data that was readily available and does not accurately represent the rest of the regions of the country. Though the data might be of high quality it cannot be used to draw conclusions about the whole country.\n\n\nData processing.\nThis phase involves getting collected data ready for processing by removing or transforming unwanted or noisy data from a dataset, classifying values, reorganizing and joining datasets collected at different regions or over different periods, and others. Analysts and data professionals processing the datasets can introduce errors and biases into the final datasets if they are not careful. For instance, they can accidentally include or exclude certain data points trying to remove noise (selection bias); introduce unintentional errors or incorrect interpretations based on their perspectives or expectations (observer bias); introduce unwanted temporal distortions, affecting the chronological order of events in the data (time-order bias).\nScenario: A group of students is tasked with processing historical annual precipitation data across different regions of the world to create input for a machine learning prediction model. When they combined datasets with precipitation data from the European continent with the dataset from the North American continent, they forgot to apply the correct daytime format. As a result, time-order bias slipped unnoticed into the final dataset.\n\n\nData analysis & modeling.\nAt this stage, data professionals create data models and evaluate their performance and interpret the outputs.\n\n\n\n\nSource: XKCD: Machine Learning\n\n\n\nThis is the phase that generates findings, which can be influenced by a variety of biases, including the two most prevalent ones: aggregation bias and confirmation bias. Aggregation Bias occurs when the data inputs contain prejudices, which are then amplified in the outputs of a model. Confirmation bias happens when a person designing a model unconsciously favors information that confirms their pre-existing beliefs or hypotheses.\n\nScenario (real-life scenario): Law enforcement agencies in the US use criminal risk assessment algorithms based on historical crime data for training predictive models. While in theory these models aim to enhance the accuracy of predicting future crimes and guide the allocation of policing resources, in practice, they rely on historical crime data with baked-in biases from policing practices‚Äîsuch as over-policing in certain neighborhoods or against specific demographics. As a result, predictions from these models can lead to an increased police presence in already over-policed areas, perpetuating a cycle of bias and potentially exacerbating social inequalities [2].\n\n\n\nPeer review.\nThough it might not be applicable to all data science projects (it should be), it‚Äôs an important phase tasked with reviewing the findings, uncovering any biases and erroneous assumptions, and suggesting improvements. If a reviewer lacks sufficient expertise or motivation, they might overlook critical issues with the analysis. Moreover, endorsing or publishing a biased study might lead to an authority bias, where the study is given more consideration than it deserves, potentially propagating incorrect conclusions into decision-making.\nScenario (real-life example): Dr.¬†Wei-Hock Soon, a scientist at the Harvard-Smithsonian Center for Astrophysics, published multiple studies showing that the sun‚Äôs energy can largely explain recent global warming. Though the study has been largely debunked, it got support from the climate denial groups and used in debates related to climate change policies [3].\n\n\nData communication.\nThis might be one the most critical phases of data science - making it comprehensible and available to the general public. This is also the stage when the facts can be easily distorted and misinterpreted. It is easy to manipulate data without committing illegal actions or presenting bogus findings. Data visualizations, in particular, can be a tool to mislead a reader intentionally or unintentionally. Common issues may include: using the wrong type of chart or graph, including too many variables, using inconsistent scale, poor color choices and others. Likewise, the omission of information perceived by a researcher or publisher as irrelevant or as giving a less favorable review of the paper can lead to the misrepresentation of important findings. This is known as reporting bias, where only a selection of results is included in an analysis, covering only a portion of the relevant evidence. Such practices can result in flawed decisions and misguided future research.\n\n\n\n\nSource: XKCD: Cell Phones\n\n\n\n\n\nData consumption.\nThis is the final phase of the data science life cycle when findings are applied to solve real-life problems and serve as input for new or follow-up studies. Here, the onus is on the user or data consumer to understand the findings correctly before drawing conclusions. Even if all the preceding steps have successfully eliminated bias, the reader may still introduce a confirmation bias when interpreting the results.\nScenario (real-life example): A reader of the study on the Covid-19 vaccine holds a preconceived belief that vaccines cause illness. After reading the study, the reader draws the conclusion that because Covid-19 cases increased around the time when the vaccine was released, the vaccine indeed caused Covid-19. This conclusion ignores the fact that other factors played a role in the increase in Covid-19 cases, such as heightened levels of testing and improved monitoring [4].\n\n\n\nAggregated view on Data Life Cycle Phases & Biases\n\n\n\n\n\n\n\n\nData Life Cycle Phase\nTasks\nCommon Biases\n\n\n\n\nProblem definition\nStates a research question, defines a problem statement, establishes objectives, and defines success criteria.\nResearcher bias\n\n\nData collection\nConduct surveys, field studies, literature review, interviews, record observations.\nSampling bias, confirmation bias, interviewer bias, overgeneralization bias, availability bias\n\n\nData processing\nClean data, classify data, remove and transform unwanted or noisy data from a dataset, reorganize, and join datasets collected at different regions or over different periods.\nSelection bias, observer bias, time-to-order bias\n\n\nData analysis & modeling\nCreate data models and evaluate their performance and interpret the outputs.\nAggregation bias, confirmation bias\n\n\nPeer review\nReview the findings, uncovering any biases and erroneous assumptions, and suggest improvements to the analysis.\nAuthority bias\n\n\nData communication\nShare the findings with the community and general public.\nReporting bias\n\n\nData consumption\nApply the findings to make decisions, solve real-life problems, and use them as input for new or follow-up studies.\nConfirmation bias\n\n\n\n\n\nConclusion\nAs we observe from our data science life cycle journey, data at every phase carries systemic biases that can easily skew findings and amplify disparities. If data professionals are not careful enough in their quest to make the world a better place, or in most cases, to simply solve mundane research and business problems, they might inadvertently exacerbate historical disparities and propagate them into the future via predictive models. Likewise, editors, peer reviewers, and the general public ought to be aware of the challenges and potential biases in the data science life cycle to become responsible consumers of data. The framework presented in this article is designed to assist in addressing these challenges and mitigating potential biases within data science, and as such to contribute to our goal of making the world a better place.\n\nCitations\n[1] How Bias Affects Scientific Research | Science News Learning. 4 Dec.¬†2020, source.\n[2] UNDPGhana. Environment and Climate Change in Ghana Policy Brief. Dec.¬†2021, source.\n[3] Gillis, Justin, and John Schwartz. ‚ÄúDeeper Ties to Corporate Cash for Doubtful Climate Researcher.‚Äù The New York Times, 21 Feb.¬†2015. NYTimes.com, source.\n[4] Maxim Lisnic, Cole Polychronis, Alexander Lex, and Marina Kogan. 2023. Misleading Beyond Visual Tricks: How People Actually Lie with Charts. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ‚Äô23), April 23‚Äì28, 2023, Hamburg, Germany. ACM, New York, NY, USA 21 Pages. source.\n\n\n\n\n\nCitationBibTeX citation:@online{protsukha2023,\n  author = {Protsukha, Oksana},\n  title = {Bias in {Data} {Science}},\n  date = {2023-12-27},\n  url = {https://oksanaprotsukha.github.io/portfolio/posts/2023-12-27-bias-in-data-science/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nProtsukha, Oksana. 2023. ‚ÄúBias in Data Science.‚Äù December\n27, 2023. https://oksanaprotsukha.github.io/portfolio/posts/2023-12-27-bias-in-data-science/."
  },
  {
    "objectID": "posts/2023-12-12-thomas-fire/thomasfire_aqi_analysis.html",
    "href": "posts/2023-12-12-thomas-fire/thomasfire_aqi_analysis.html",
    "title": "Impact of the Thomas Fire on AQI in Santa Barbara County, CA (2017)",
    "section": "",
    "text": "Author: Oksana Protsukha\nGithub repository: thomasfire_aqi_analysis\n\n\nIn this notebook, we are going to analyze the impact of Thomas Fire on the air quality in Santa Barbara county in 2017. To do so, we will complete the following two tasks:  1. Visualize the Air Quality Index (AQI) in Santa Barbara County for 2017 and 2018. 2. Create a false color image to visualize the Thomas Fire scar.\n\n\nThomas Fire started on December 4, north of Santa Paula, CA, and was contained only on January 12, 2018. The fire, spread by the unusually strong and persistent Santa Ana winds, destroyed 281,893 acres (440 sq mi; 114,078 ha) and threatened neighborhoods throughout Santa Barbara and Ventura counties. During this period, air quality significantly degraded to levels considered ‚Äúvery unhealthy‚Äù (201-300) by EPA standards. You can read about AIQ at AirNow website. \nFalse Color Image allows visualizing objects on the images that were generated via remote sensing using the wavelengths that fall outside the visual light spectrum: - Near-infrared (NIR) - Shortwave infrared (SWIR) - Midwave Infrared (MIR) - Infrared (IR) - Thermal or longwave infrared (TIR or LWIR) Assignment of these wavelegnths to different colors helps to highlight certain characteristics of an object that may not be visible in a true-color image.\nI highly recommend NASA website to read more about the false color imagery: NASA Earth Observatory\n\n\n\nThe primary purpose of the notebook is to demonstrate environmental data analysis methods using both vector and raster data.\n\n\n\n\nPython libraries\n\nnumpy\npandas\npyplot and patches from matplotlib\nrioxarray\ngeopandas\nxarray\n\nJupyter Notebook\nGitHub\n\n\n\n\n\nData wrangling and exploration with Pandas\nGeospatial data wrangling with GeoPandas and Rioxarray\nTime series analysis\nMerging tabular and vector data\nVisualizing analysis results with a line plot\nCreating and customizing a map with raster and vector data\n\n\n\n\n\n\nAir Quality Index (AQI) data from the US Environmental Protection Agency[1] to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County [2].\n\n\n\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite [3]. Note: data should be used for visualization purposes only.\nInformation about Landsat bands from USGS:\n\nWhat are the band designations for the Landsat satellites?\nCommon Landsat Band Combinations\nHow do I use a scale factor with Landsat Level-2 science products?\n\n\n\n\nA prepared shapefile of fire perimeters in California during 2017. The complete file can be accessed in the CA state geoportal[4].\nData for datasets 2 and 3 is available on Google Drive in the file thomasfire_aqi_analysis.zip.\n\n\n\n\nThe final outputs of the analysis include:\nA Line Plot depicting the time series analysis of the Air Quality Index (AQI) in Santa Barbara for 2017 and 2018. \nA Map with an overlay of the Thomas Fire scar on the Santa Barbara land cover."
  },
  {
    "objectID": "posts/2023-12-12-thomas-fire/thomasfire_aqi_analysis.html#overview",
    "href": "posts/2023-12-12-thomas-fire/thomasfire_aqi_analysis.html#overview",
    "title": "Impact of the Thomas Fire on AQI in Santa Barbara County, CA (2017)",
    "section": "",
    "text": "In this notebook, we are going to analyze the impact of Thomas Fire on the air quality in Santa Barbara county in 2017. To do so, we will complete the following two tasks:  1. Visualize the Air Quality Index (AQI) in Santa Barbara County for 2017 and 2018. 2. Create a false color image to visualize the Thomas Fire scar.\n\n\nThomas Fire started on December 4, north of Santa Paula, CA, and was contained only on January 12, 2018. The fire, spread by the unusually strong and persistent Santa Ana winds, destroyed 281,893 acres (440 sq mi; 114,078 ha) and threatened neighborhoods throughout Santa Barbara and Ventura counties. During this period, air quality significantly degraded to levels considered ‚Äúvery unhealthy‚Äù (201-300) by EPA standards. You can read about AIQ at AirNow website. \nFalse Color Image allows visualizing objects on the images that were generated via remote sensing using the wavelengths that fall outside the visual light spectrum: - Near-infrared (NIR) - Shortwave infrared (SWIR) - Midwave Infrared (MIR) - Infrared (IR) - Thermal or longwave infrared (TIR or LWIR) Assignment of these wavelegnths to different colors helps to highlight certain characteristics of an object that may not be visible in a true-color image.\nI highly recommend NASA website to read more about the false color imagery: NASA Earth Observatory\n\n\n\nThe primary purpose of the notebook is to demonstrate environmental data analysis methods using both vector and raster data.\n\n\n\n\nPython libraries\n\nnumpy\npandas\npyplot and patches from matplotlib\nrioxarray\ngeopandas\nxarray\n\nJupyter Notebook\nGitHub\n\n\n\n\n\nData wrangling and exploration with Pandas\nGeospatial data wrangling with GeoPandas and Rioxarray\nTime series analysis\nMerging tabular and vector data\nVisualizing analysis results with a line plot\nCreating and customizing a map with raster and vector data\n\n\n\n\n\n\nAir Quality Index (AQI) data from the US Environmental Protection Agency[1] to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County [2].\n\n\n\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite [3]. Note: data should be used for visualization purposes only.\nInformation about Landsat bands from USGS:\n\nWhat are the band designations for the Landsat satellites?\nCommon Landsat Band Combinations\nHow do I use a scale factor with Landsat Level-2 science products?\n\n\n\n\nA prepared shapefile of fire perimeters in California during 2017. The complete file can be accessed in the CA state geoportal[4].\nData for datasets 2 and 3 is available on Google Drive in the file thomasfire_aqi_analysis.zip.\n\n\n\n\nThe final outputs of the analysis include:\nA Line Plot depicting the time series analysis of the Air Quality Index (AQI) in Santa Barbara for 2017 and 2018. \nA Map with an overlay of the Thomas Fire scar on the Santa Barbara land cover."
  },
  {
    "objectID": "posts/2023-12-12-thomas-fire/thomasfire_aqi_analysis.html#visualize-aqi-in-santa-barbara-county-during-thomas-fire-2017",
    "href": "posts/2023-12-12-thomas-fire/thomasfire_aqi_analysis.html#visualize-aqi-in-santa-barbara-county-during-thomas-fire-2017",
    "title": "Impact of the Thomas Fire on AQI in Santa Barbara County, CA (2017)",
    "section": "Visualize AQI in Santa Barbara County during Thomas Fire (2017)",
    "text": "Visualize AQI in Santa Barbara County during Thomas Fire (2017)\n\nImport Air quality data \nWe are going to import AIQ data by county for 2017 and 2018.\n\n\nCode\n# import Daily AQI by County data from url: Dataset 1\n# read in 2017 Daily AQI by County\nurl_aqi_17 = 'https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip'\naqi_17 = pd.read_csv(url_aqi_17)\n\n# read in 2018 Daily AQI by County\nurl_aqi_18 = 'https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip'\naqi_18 = pd.read_csv(url_aqi_18)\n\n\n\n\nPrepare Air quality data\n\n\nJoin dataframes\nWe currently have two separate dataframes for 2017 and 2018. However, in order to analyze AQI for the two years, we need to combine them by appending the rows from one dataframe on top of the other. We will use the pandas function pd.concat() to do so.\nImportant: in order to combine the two dataframes their columns have to match.\n\n\nCode\n%%capture\n\n# check whether the columns between the two datasets match\nprint(f'Do columns match?\\n {aqi_18.columns == aqi_17.columns} \\n')\n\n# check whether the column datatypes between the two datasets match\nprint(f'Do datatypes of the columns match?\\n{aqi_18.dtypes == aqi_17.dtypes}')\n\n\nNote: Since the column names and data types of the two datasets match exactly, we can combine them into a single dataframe. To ensure data quality, we will verify that the combined dataframe has a total number of rows equal to the sum of the records from the two separate datasets.\n\n\nCode\n%%capture\n\n# combine two datasets aqi_17 and aqi_18 in a single one\naqi = pd.concat([aqi_17, aqi_18])\n\n# check the total count of rows matches the expected number\nprint(f'Is the total number of rows in the combined dataframe as expected? {len(aqi) == (len(aqi_17)+len(aqi_18))}')\n\n# check the first 2 rows of the combined dataframe\naqi.head(2)\n\n\n\n\nData wrangling\n\nFilter dataframe\nFor our analysis we need only AQI data from the Santa Barbara county. Let‚Äôs create a new dataframeaqi_sb filtered to Santa Barbara county.\nAdditionally, we will remove unecessary columns state_name, county_name, state_code and county_code from the new dataframe and update the column names to snake_case üêç to simplify data manipulation.\n\n\nCode\n%%capture\n\n# update the column names case to snake_case\naqi.columns = aqi.columns.str.lower().str.replace(' ','_')\n\n# select the records where county_name = 'Santa Barbara' from the combined dataset aqi\naqi_sb = aqi.loc[aqi.county_name == 'Santa Barbara'].copy()\n\n# remove the state_name, county_name, state_code and county_code columns from aqi_sb\naqi_sb = aqi_sb.drop(columns = ['state_name', 'county_name', 'state_code', 'county_code'])\n\n\n\n\nSet date column as the index\nIn our analysis, we will examine data over time. Having a date column in the datetime format, configured as an index, enables the use of built-in functions when working with dates.\n\n\nCode\n# check the data types of the columns\nprint(aqi_sb.dtypes)\n\n\ndate                         object\naqi                           int64\ncategory                     object\ndefining_parameter           object\ndefining_site                object\nnumber_of_sites_reporting     int64\ndtype: object\n\n\n‚ö†Ô∏è Date column has type object rather than datetime as expected.\nWe will make the following changes to facilitate working with dates:\n\nConvert the date column of aqi_sb to a datetime object.\nSet the date column as the index for aqi_sb dataframe.\n\n\n\nCode\n%%capture\n\n# update date column values of aqi_sb to datetype object\naqi_sb.date = pd.to_datetime(aqi_sb.date)\n      \n# update the index of aqi_sb to be the date column\naqi_sb = aqi_sb.set_index('date')\n\n# check column date is updated\nprint(aqi_sb.index)\n\n\n\n\nAdd a rolling average column\nOur current dataframe contains daily values of AQI, which can create short-term fluctuations and noise in data. We‚Äôll apply a 5-day rolling average to smooth out the data and improve the quality of the analysis. To achieve this we will create a new variable, five_day_average, using the pandas method rolling.\nNote: rolling() is a method for pandas.series that provides rolling window calculations. This is a lazy method (think groupby), we need to specify what we want to calculate over each window. It returns pd.Series as ouput.\nLet‚Äôs update and view first few rows of the final dataframe. We will use this dataframe to visualize AQI in Santa Barbara county during 2017 and 2018.\n\n\nCode\n# calculate 5-day rolling average AQI\n# the parameter '5D' indicates we want the window to be 5 days\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean().round(2) \n\n# final dataframe for visualization\nfinal_table = PrettyTable()\n# add columns to the table\nfinal_table.field_names = aqi_sb.columns.tolist()\n\n# subset_df = pd.concat([aqi_sb.head(3), aqi_sb.tail(3)])\n\n# Add rows to the table\nfor _, row in aqi_sb.head(5).iterrows():\n    final_table.add_row(row.tolist())\n\n#PrettyTable(aqi_sb.head(2))\nprint(final_table)\n\n\n+-----+----------+--------------------+---------------+---------------------------+------------------+\n| aqi | category | defining_parameter | defining_site | number_of_sites_reporting | five_day_average |\n+-----+----------+--------------------+---------------+---------------------------+------------------+\n|  39 |   Good   |       Ozone        |  06-083-4003  |             12            |       39.0       |\n|  36 |   Good   |       Ozone        |  06-083-4003  |             11            |       37.5       |\n|  71 | Moderate |        PM10        |  06-083-4003  |             12            |      48.67       |\n|  34 |   Good   |       Ozone        |  06-083-4003  |             13            |       45.0       |\n|  37 |   Good   |       Ozone        |  06-083-4003  |             12            |       43.4       |\n+-----+----------+--------------------+---------------+---------------------------+------------------+\n\n\n\n\n\nVisualize AQI output \nIn the next step, we will create a line plot with daily AQI and 5-day average AQI to visualize the impact of the Thomas Fire (December 2017) on air quality in Santa Barbara County.\n\n\nCode\n# plot both the daily AQI and the 5-day average\nfig, ax = plt.subplots()\n\naqi_sb.aqi.plot(ax=ax,\n                color = '#fecc5c')\naqi_sb.five_day_average.plot(ax=ax,\n                color = '#a50f15')\n\n# update axis\nax.set_title('Daily Average vs 5 day Average AQI in Santa Barbara (2017-2018)')\nax.set_xlabel('Period')\nax.set_ylabel('AQI')\nax.legend(['AQI Daily Average', 'AQI 5-day Average'])\n\n# annotate the data source\nax.annotate(\"Data: AirData Website File Download Page.\\nhttps://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed 28 Nov. 2023\",\n            xy=(0.5, -0.15), \n            fontsize=7, \n            xycoords='axes fraction',  \n            color='#555555',\n            ha='center',\n            va='top')  \n\n# adjust the botton space of the figure\nplt.subplots_adjust(bottom=0.01)\n\nplt.show\n\n\n&lt;function matplotlib.pyplot.show(close=None, block=None)&gt;\n\n\n\n\n\n\nInterpretation\nFrom the graph we can see a significant increase in both the daily average and 5-day rolling average in December 2017 compared to the rest of the year, which coincides with the Thomas Fire event. During this period, the AQI exceeded 200, falling into the very unhealthy category. According to AQI standards, at this level, the risk of health effects is increased for everyone."
  },
  {
    "objectID": "posts/2023-12-12-thomas-fire/thomasfire_aqi_analysis.html#create-a-false-color-image-to-visualize-the-thomas-fire-scar",
    "href": "posts/2023-12-12-thomas-fire/thomasfire_aqi_analysis.html#create-a-false-color-image-to-visualize-the-thomas-fire-scar",
    "title": "Impact of the Thomas Fire on AQI in Santa Barbara County, CA (2017)",
    "section": "Create a false color image to visualize the Thomas Fire scar",
    "text": "Create a false color image to visualize the Thomas Fire scar\n\nImport geospatial data\nData for the following two datasets is available on Google Drive in the file: thomasfire_aqi_analysis.zip.\n\nImport Landsat geospatial data (netCDF)\n\n\nCode\n# use rioxarray library to import netCDF file with geospatial features\nfp_lsat = 'data/landsat8-2018-01-26-sb-simplified.nc'\nlsat = rioxr.open_rasterio(fp_lsat)\n\n\n\n\nImport California fire perimeter shape file\n\n\nCode\n# use geopandas to import shape file with California fire perimeters \ncalfire = gpd.read_file(os.path.join(os.getcwd(), 'data', \n                                     'California_Fire_Perimeters_2017',\n                                     'California_Fire_Perimeters_2017.shp'))  \n\n\n\n\n\nPrepare geospatial data \nFirst, let‚Äôs inspect geospatial datasets.\n\n\nCode\n%%capture\n\n# explore shape file\n\n# view shape and data type\nprint(f'calfire dataset shape: {calfire.shape}')\nprint(f'calfire dataset crs: {calfire.crs}')\n\n# view the first two records of imported shape file data\ncalfire.head(2)\n\n# visualize calfire data\ncalfire.plot(color = 'none')\n\n\n\nCreate a new dataframe with Thomas Fire scar geometries\nCalifornia fire perimeter dataset includes the list of all fires in California during 2017. For our analysis, we are interested only in Thomas fire that occured that year.\nTherefore, we need to filter the dataset to the rows with Thomas fire.\nNote: We only have one record with fire_name = \"THOMAS\" in the dataframe. This is the record that contains the geometries of the Thomas Fire scar.\n\n\nCode\n# update the column names case to snake_case\ncalfire.columns = calfire.columns.str.lower().str.replace(' ','_')\n\n# find the value that corresponds to Thomas fire\ncalfire[calfire.fire_name.str.contains('Thomas', case=False, na=False)]['fire_name'].unique()\n\n# create a new dataframe with Thomas fire details\ntf = calfire[calfire.fire_name == 'THOMAS']\n# keep only required fields\ntf = tf.loc[:, ['fire_name', 'gis_acres', 'shape_leng','shape_area','geometry']]\n\n\n\n\nReproject CRS\n‚ö†Ô∏è Important:\n\nIn order to overlay the Thomas Fire perimeter on the Santa Barbara County land cover, we need to ensure that the CRSs of the two datasets are the same. To achieve this, we will reproject the CRS of the tf dataset into the CRS of the lsat dataset.\nThe lsat dataset includes an additional dimension, the band, which makes it a 3-dimensional object. We are going to convert it into a 2-dimensional dataset by dropping the band dimension.\n\n\n\nCode\n%%capture\n\n# check that crs of the shape file and netCDF match\ncalfire.crs == lsat.rio.crs\n\n# update CRS of the shape file to lsat CRS\ntf = tf.to_crs(crs = lsat.rio.crs)\nprint(f'CRS of the lsat and tf datasets match: {tf.crs == lsat.rio.crs}')\n\n# drop an extra dimension \"band\" from lsat dataset\nlsat = lsat.squeeze().drop('band')\n# check that band dimension has been dropped\nprint(f'After squeeze:\\ndimensions {lsat.dims}\\ncoords: {lsat.coords}')\n\n\n\n\n\nCreate a false color image of Thomas Fire Scar\nIn the final step we will create a map showing the shortwave infrared/nir/ ed false color image together with the Thomas fire perimeter.\n\n\nCode\nfig, ax = plt.subplots()\n\n# remove axis\nax.axis('off')\n\n# plot Thomas fire perimeter\ntf.plot(ax=ax, color='none', edgecolor = '#483C32')\ntf_patch = mpatches.Patch(facecolor = 'none', \n                          edgecolor='#483C32', \n                          label='Thomas Fire Perimeter (2017)')\n\n# plot raster image of Santa Barbara area\nlsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(ax=ax, robust = True)\n\n# add legend\nax.legend(handles=[tf_patch], frameon=True, loc='upper right')\nax.set_title('Thomas Fire (Santa Barbara, 2017)')\n\n# annotate the data source\nax.annotate(\"Data: Microsoft Planetary Computer.https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed 28 Nov. 2023.\\nCalifornia Fire Perimeters (All).https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed 28 Nov. 2023.\",\n            xy=(0.5, -0.15),  # Move the annotation lower to leave space for the x-axis title\n            fontsize=7, \n            xycoords='axes fraction',  \n            color='#555555',\n            ha='center')  \n\n# adjust the botton space of the figure\nplt.subplots_adjust(bottom=0.01)\n\n# display map\nplt.show()\n\n\n\n\n\n\n\nCitations:\n[1] AirData Website File Download Page. https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI. Accessed 28 Nov.¬†2023. [2] ‚ÄúThomas Fire.‚Äù Wikipedia, 9 Nov.¬†2023. Wikipedia, https://en.wikipedia.org/w/index.php?title=Thomas_Fire&oldid=1184323284. [3] Microsoft Planetary Computer. https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2. Accessed 28 Nov.¬†2023. [4] California Fire Perimeters (All). https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about. Accessed 28 Nov.¬†2023."
  },
  {
    "objectID": "posts/2024-03-15-se-ev-equity-data-viz/index.html",
    "href": "posts/2024-03-15-se-ev-equity-data-viz/index.html",
    "title": "Is EV Charging Infrastructure Equitable?",
    "section": "",
    "text": "In this blog post, we will create three charts to investigate the accessibility of charging infrastructure for disadvantaged communities. Before we proceed to the data and code, let‚Äôs consider what makes this topic important.\nWith a rapid transition to ‚Äúgreen economy‚Äù EV charging infrastructure has come into the spotlight. Initiatives like California‚Äôs adoption of ‚ÄúThe Advanced Clean Cars II‚Äù in 2022 mandating all new vehicle sales in California be zero-emissions vehicles (ZEV) by 2035, further turned attention to the availability of charging infrastructure. As states like Rhode Island, Washington, Virginia, Vermont, Oregon, New York, and Massachusetts adopted similar mandates and Connecticut, Colorado, Delaware, Maryland, New Jersey, New Mexico, Minnesota, and Nevada are expected to follow suit, the question in inclusivity of this transition becomes ever important. Will disadvantaged communities have equitable access to the benefits of the green economy, or will they be left behind? While high EV prices and limited availability remain primary obstacles, the lack of accessible charging infrastructure poses a challenge of its own. Insufficient charging infrastructure charging in these communities can discourage the households from adopting EVs. Furthermore, charging infrastructure offers multiple benefits to the communities where they are implemented, such as improved air quality, increased business opportunities, and higher real estate values, and etc. (refer to DOE website. Disadvantaged communities, in particular, stand to gain from these improvements."
  },
  {
    "objectID": "posts/2024-03-15-se-ev-equity-data-viz/index.html#background",
    "href": "posts/2024-03-15-se-ev-equity-data-viz/index.html#background",
    "title": "Is EV Charging Infrastructure Equitable?",
    "section": "",
    "text": "In this blog post, we will create three charts to investigate the accessibility of charging infrastructure for disadvantaged communities. Before we proceed to the data and code, let‚Äôs consider what makes this topic important.\nWith a rapid transition to ‚Äúgreen economy‚Äù EV charging infrastructure has come into the spotlight. Initiatives like California‚Äôs adoption of ‚ÄúThe Advanced Clean Cars II‚Äù in 2022 mandating all new vehicle sales in California be zero-emissions vehicles (ZEV) by 2035, further turned attention to the availability of charging infrastructure. As states like Rhode Island, Washington, Virginia, Vermont, Oregon, New York, and Massachusetts adopted similar mandates and Connecticut, Colorado, Delaware, Maryland, New Jersey, New Mexico, Minnesota, and Nevada are expected to follow suit, the question in inclusivity of this transition becomes ever important. Will disadvantaged communities have equitable access to the benefits of the green economy, or will they be left behind? While high EV prices and limited availability remain primary obstacles, the lack of accessible charging infrastructure poses a challenge of its own. Insufficient charging infrastructure charging in these communities can discourage the households from adopting EVs. Furthermore, charging infrastructure offers multiple benefits to the communities where they are implemented, such as improved air quality, increased business opportunities, and higher real estate values, and etc. (refer to DOE website. Disadvantaged communities, in particular, stand to gain from these improvements."
  },
  {
    "objectID": "posts/2024-03-15-se-ev-equity-data-viz/index.html#data-and-design-process",
    "href": "posts/2024-03-15-se-ev-equity-data-viz/index.html#data-and-design-process",
    "title": "Is EV Charging Infrastructure Equitable?",
    "section": "Data And Design process",
    "text": "Data And Design process\n\nData\nI used the following data sets to address the question about equity within EV charging network distribution and deployment in the United States:\n\nDataset 2. Climate and Economic Justice Screening Tool.¬† Source: CEJST\n\n\n\n\n\n\n\nLearn about the dataset\n\n\n\nThe dataset highlights disadvantaged census tracts across the United States, and the U.S. territories for the following categories: climate change, energy, health, housing, legacy pollution, transportation, water and wastewater, and workforce development. For this infographic I used transportation categories, as well as the community‚Äôs status as disadvantaged or not. Overview and metadata details:\n    a.  [Description of the dataset](https://static-data-screeningtool.geoplatform.gov/data-versions/1.0/data/score/downloadable/Using-the-CEJST-Spreadsheet-Tutorial.pdf).\n    b.  [Technical documentation and methodology](https://screeningtool.geoplatform.gov/en/methodology).\n    c.  [the GitHub repository](https://github.com/usds/justice40-tool/blob/main/DATASETS.md).\n\n\nClimate and Economic Justice Tool data set variables used in modeling:\n\nSF: State/Territory\nCF: County Name\nGEOID: Census Tract\nTPF: Total population\nTF_PFS: Traffic proximity and volume (percentile)\nTP_ET: Greater than or equal to the 90th percentile for traffic proximity\nSN_C: Definition N community, including adjacency index tracts\n\n\nDataset 2. Alternative Fuels Data Center: Electric Vehicle Charging Station Locations.¬† Source: NREL\n\n\n\n\n\n\n\nLearn about the dataset\n\n\n\nThe data set is accessible via NREL maintained API and provides a list of over 70 features related to alternative fuels alternative fuels stations. For this infographic I used a subset of attributes specific to electric vehicle charging stations.\n\n\nNREL API data set variables used in modeling:\n\nid: EV charger ID\nstatus_code: EV charger status (available, planned)\nev_dc_fast_num: DC fast charging port type\nev_level2_evse_num: Level 2 charging port type\nlongtitude and latitude: position of an EV charging station\n\n\n\n\n\n\n\n\nTypes of charging ports\n\n\n\n\nLevel 1 connector (out of scope from the analysis): standardized connector primarily for home use.\nThe charge rate is 3-5 miles per hour.\nApplicable for trips up to 40 miles or less daily.\n\nLevel 2 charging port: standardized connector primarily for home or office use.\nThe charge rate is 15-30 miles per hour.\nApplicable for trips 100+ miles daily.\n\nDC fast charging port: three connector types (CCS, CHAdeMO, NACS) connected to dedicated high power stations.\nThe charge rate is 150-400 miles per hour.\nApplicable for trips 300+ miles daily.\n\n\n\n\n\n\nDesign Decisions\n\nDefine a primary and leading questions to visualize.\nMy infographic explores the question, ‚ÄúIs our EV charging infrastructure equitable?‚Äù. To address this complex primary question, I divided the analysis into three separate leading questions.\n\nQuestion 1: Which of the top 5 states have the most number of EV charging ports installed or in progress to date? This helped me to target my analysis on the states that have the most data to work with.\nQuestion 2: Among the top 5 states from Question 1, how do the ratios of EV charging ports to state population compare with those within disadvantaged communities in the 90th percentile of traffic proximity? These communities, being near major roads with heavy traffic, are ideal candidates for EV charging infrastructure deployment due to high demand.\nQuestion 3: In states where disadvantaged communities in the 90th percentile of traffic proximity have fewer EV charging ports per capita compared to the state average, can we see a noticeable bias in the distribution of EV charging stations on the map?\n\nSelect the graphic form to visualize each question presented in the infographic.\n\nFor Question 1, I chose a sunburst plot to display the number of EV chargers in each state. The choice was informed by the compact layout of the sunburst plot and its ability to visualize distribution across multiple sections without overcrowding a figure. Additionally, the sunburst plot allows zooming into individual states to access statistics. Note: the latter feature is available in the interactive mode only.\nFor Question 2, I selected a dumbbell plot for its effectiveness in comparing two values measured in the same units. It offers a compact layout and ease of interpretation, making it the preferred choice for visualizing the second question.\nFor Question 3, I used a choropleth-style map with an inset. I made this choice to visualize the distribution of EV charging stations within different types of census tracts across the state.\n\nDecide on the design approach (theme, fonts, colors, etc.).\n\nText (e.g.¬†titles, captions, annotations, axis labels, axis ticks, alt text). For the first and second figures, I included the titles, while I relied on the legend to explain the choropleth map. I excluded the axis titles from all graphs as it was not necessary to understand the content. This allowed me to maintain clean layouts.\nThemes (i.e.¬†all non-data plot elements). I used theme_bw and theme_void to keep the visuals clean by removing unnecessary elements such as axes from the choropleth maps or grids from the dumbbell plot.\nColors. I opted for a pastel palette of green, pink, and yellow.\n\nGreen color was used to symbolically display EV charger locations, highlighting their contrbution towards a ‚Äúgreen‚Äù economy.\nMaroon color with a shade of brown was chosen to represent disadvantaged communities, subtly conveying a sense of hardship or challenges.\nLight yellow was chosen as the background color, complementing the visual elements and ensuring sufficient contrast between them.\n\nTypography. I used default fonts in my infographic to keep the layout simple and avoid distractions from the main questions. Where appropriate, I used a bold fontface and changed the size of the fonts for readability.\nGeneral design (e.g.¬†group order, spacing, text orientation, data-ink ratio, creating a visual hierarchy, avoiding information overload). In my infographic, my aim was to guide viewers towards answering the main question, ‚ÄúIs EV infrastructure in the United States equitable?‚Äù To achieve this, I positioned the figures to transition from an overview (sunburst plot) to more focused details (choropleth maps).\n\nDecide how to center your primary message on the infographic.\nThe primary figure‚Äîa choropleth map‚Äîdisplays the distribution of EV chargers relative to disadvantaged communities. While it is the most critical element in answering the main question, I placed it below the other two figures to maintain a logical sequence in the narrative.\nConsider accessibility (e.g.¬†colorblind-friendly palettes / contrast, alt text).\nI attempted to use colors that are accessible for viewers who have color-blindness conditions. While for most cases of color-blindness, I was able to achieve a sufficient level of contrast to view the figures, in few instances, such as monochromacy and protanipia, the contrast is lacking. I used an online color blindness simulator to verify the figures for color-blindness. To address these cases, I included an alt-text to all three visualizations.\nApply a Diversity, Equity, and Inclusion (DEI) lens to the design (e.g.¬†considering the people / communities / places represented in the data, framing of the questions).\nAs my infographic focused on identifying bias in the deployment of EV charging infrastructure concerning disadvantaged communities, I aimed to maintain an objective tone in both my visuals and text. Specifically, I utilized a federally sponsored dataset, the ‚ÄúClimate and Economic Screening Tool,‚Äù to identify disadvantaged communities."
  },
  {
    "objectID": "src/import-ev-charging-data.html",
    "href": "src/import-ev-charging-data.html",
    "title": "Oksana Protsukha",
    "section": "",
    "text": "# load packages\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.3     ‚úî readr     2.1.4\n‚úî forcats   1.0.0     ‚úî stringr   1.5.0\n‚úî ggplot2   3.4.4     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.0\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter()  masks stats::filter()\n‚úñ purrr::flatten() masks jsonlite::flatten()\n‚úñ dplyr::lag()     masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readr)\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n\n\n# load configuration\nhere::set_here(path='/Users/oksi/Documents/UCSB/MEDS/EDS-240/assignments/eds240-HW4/', verbose = TRUE)\n\nFile .here already exists in /Users/oksi/Documents/UCSB/MEDS/EDS-240/assignments/eds240-HW4\n\nsource(file.path(here::here('src','config.R')))\n\n# retrieve API Key\napi_key &lt;- NREL_API_KEY\n\n\n# get charging stations for United States\n# specify API endpoint and parameters\napi_url &lt;- \"https://developer.nrel.gov/api/alt-fuel-stations/v1\"\n\napi_params &lt;- list(\n  format = \"json\",  \n  api_key = api_key, \n  status = paste(c(\"E\", \"P\"), collapse = \",\"), # E = charging stations in operation; P = planned\n  access = \"public\",\n  fuel_type = \"ELEC\",\n  cng_vehicle_class = \"LD\", # LD = light duty vehicles\n  country = \"US\",\n  limit = \"all\"\n)\n\n# construct the full URL with parameters\nrequest_url &lt;- modify_url(api_url, query = api_params)\n\n# make a GET request to the API\nresponse &lt;- GET(request_url)\n\n# extract the fuel_stations data from the response\nfuel_stations_us_df &lt;- content(response, \"parsed\")$fuel_stations\n\n# convert the list to a dataframe\nfuel_stations_us_df &lt;- as.data.frame(do.call(rbind, fuel_stations_us_df))\n\nWarning in (function (..., deparse.level = 1) : number of columns of result is\nnot a multiple of vector length (arg 1)\n\n# select the fields for analysis\nfuel_stations_us_df_clean &lt;- fuel_stations_us_df %&gt;% \n  filter(!is.na(fuel_type_code)) %&gt;% \n  select('id', 'status_code', 'access_code','owner_type_code', 'open_date', 'restricted_access', 'maximum_vehicle_class','facility_type', 'city','state','zip','ev_workplace_charging', 'ev_level1_evse_num', 'ev_level2_evse_num', 'ev_dc_fast_num' , 'longitude', 'latitude')\n\n# flatten data frame - NOT WORKING CODE\n# fuel_stations_us_df_clean &lt;- fuel_stations_us_df_clean %&gt;%\n#   mutate(across(where(is.list), unnest))\n\n# convert data frame to sf object\nus_ev_sf &lt;- fuel_stations_us_df_clean %&gt;%\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)"
  },
  {
    "objectID": "posts/2024-03-15-se-ev-equity-data-viz/index.html#code",
    "href": "posts/2024-03-15-se-ev-equity-data-viz/index.html#code",
    "title": "Is EV Charging Infrastructure Equitable?",
    "section": "Code",
    "text": "Code\n\nSetup\n\n\nCode\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n## load packages\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(showtext)\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(ggridges)\nlibrary(data.table)\nlibrary(plotly)\nlibrary(ggimage)\nlibrary(tigris)\nlibrary(cowplot)\nlibrary(here)\n\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n## import and process data\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nsource(file.path(rootdir, \"src\", \"prep.R\"))\n\n\n\n\nVisualizations\n\nVisualization 1: Sunburst chart\n\n\nCode\n# --- Data Wrangling ---#\n\n# define regions\nregions &lt;- c(rep(\"Midwest\", 12), rep(\"Northeast\", 12), rep(\"Southeast\", 12), rep(\"Southwest\", 4), rep(\"West\", 11))\n\n# define states\nstates &lt;- c(\n  rep(c(\"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Nebraska\", \"North Dakota\", \"Ohio\", \"South Dakota\", \"Wisconsin\"), 1),\n  rep(c(\"Connecticut\", \"Maryland\", \"Maine\", \"New York\", \"Pennsylvania\", \"Vermont\", \"Delaware\", \"Massachusetts\", \"New Jersey\", \"New Hampshire\", \"Rhode Island\", \"District of Columbia\"), 1),\n  rep(c(\"Arkansas\", \"Louisiana\", \"Kentucky\", \"Tennessee\", \"Mississippi\", \"Alabama\", \"West Virginia\", \"Virginia\", \"North Carolina\", \"South Carolina\", \"Georgia\", \"Florida\"), 1),\n  rep(c(\"Arizona\", \"New Mexico\", \"Texas\", \"Oklahoma\"), 1),\n  rep(c(\"Alaska\", \"California\", \"Colorado\", \"Hawaii\", \"Idaho\", \"Montana\", \"Nevada\", \"Oregon\", \"Utah\", \"Washington\", \"Wyoming\"), 1)\n)\n\n# define ev_chargers deployed within each state\nev_ports &lt;- c(\n  3252, 1489, 815, 1091, 3210, 1994, 2471, 550, 204, 3553, 226, 1393, 2248, 4660, 1050, 10613, 4161, 938, 512, 7038, 3488, 536, 649, 1053, 807, 650, 783, 1979, 396, 946, 419, 4125, 3969, 1351, 4869, 9560, 3435, 669, 9539, 1286, 119, 44034, 5294, 781, 457, 380, 1886, 2976, 2217, 5501, 238\n)\n\n# create the dataframe\nev_distribution_df &lt;- data.frame(\n  regions = regions,\n  states = states,\n  ev_ports = ev_ports\n)\n\n\n# --- Hierarchy for sunburst plot ---# \n\n# function to generate sunburst data hierarchy\nas.sunburstDF &lt;- function(DF, value_column = NULL, add_root = FALSE){\n  # load data.table library to hanlde enhanced data.frame functionality\n  require(data.table)\n  \n  colNamesDF &lt;- names(DF)\n  \n  if(is.data.table(DF)){\n    DT &lt;- copy(DF)\n  } else {\n    DT &lt;- data.table(DF, stringsAsFactors = FALSE)\n  }\n  \n  if(add_root){\n    DT[, root := \"Total\"]  \n  }\n  \n  colNamesDT &lt;- names(DT)\n  hierarchy_columns &lt;- setdiff(colNamesDT, value_column)\n  DT[, (hierarchy_columns) := lapply(.SD, as.factor), .SDcols = hierarchy_columns]\n  \n  if(is.null(value_column) && add_root){\n    setcolorder(DT, c(\"root\", colNamesDF))\n  } else if(!is.null(value_column) && !add_root) {\n    setnames(DT, value_column, \"values\", skip_absent=TRUE)\n    setcolorder(DT, c(setdiff(colNamesDF, value_column), \"values\"))\n  } else if(!is.null(value_column) && add_root) {\n    setnames(DT, value_column, \"values\", skip_absent=TRUE)\n    setcolorder(DT, c(\"root\", setdiff(colNamesDF, value_column), \"values\"))\n  }\n  \n  hierarchyList &lt;- list()\n  \n  for(i in seq_along(hierarchy_columns)){\n    current_columns &lt;- colNamesDT[1:i]\n    if(is.null(value_column)){\n      currentDT &lt;- unique(DT[, ..current_columns][, values := .N, by = current_columns], by = current_columns)\n    } else {\n      currentDT &lt;- DT[, lapply(.SD, sum, na.rm = TRUE), by=current_columns, .SDcols = \"values\"]\n    }\n    setnames(currentDT, length(current_columns), \"labels\")\n    hierarchyList[[i]] &lt;- currentDT\n  }\n  \n  hierarchyDT &lt;- rbindlist(hierarchyList, use.names = TRUE, fill = TRUE)\n  \n  parent_columns &lt;- setdiff(names(hierarchyDT), c(\"labels\", \"values\", value_column))\n  hierarchyDT[, parents := apply(.SD, 1, function(x){fifelse(all(is.na(x)), yes = NA_character_, no = paste(x[!is.na(x)], sep = \":\", collapse = \" - \"))}), .SDcols = parent_columns]\n  hierarchyDT[, ids := apply(.SD, 1, function(x){paste(x[!is.na(x)], collapse = \" - \")}), .SDcols = c(\"parents\", \"labels\")]\n  hierarchyDT[, c(parent_columns) := NULL]\n  return(hierarchyDT)\n}\n\nsunburstDF &lt;- as.sunburstDF(ev_distribution_df, value_column = \"ev_ports\", add_root = TRUE)\n\n# head(sunburstDF)\n\n\n# --- SUnburst Plot --- #\n\n# create labels for a sunburst plot\nselected_states &lt;- ev_distribution_df %&gt;% \n  select(states, ev_ports) %&gt;%\n  distinct() %&gt;% \n  slice_max(order_by = ev_ports, n = 5)\n\nselected_states &lt;- selected_states$states\n\n# create text vector for selected states\ntext_selected &lt;- ifelse(sunburstDF$labels %in% c(\"Midwest\", \"Northeast\", \"Southeast\", \"Southwest\", \"West\", \"Total\"), \n               paste(scales::comma(sunburstDF$values), sep = \" \"),  # combine labels with their corresponding values for regions and totals\n               ifelse(sunburstDF$labels %in% selected_states, \n                      scales::comma(sunburstDF$values),  # include values for selected states\n                      \"\"))  # set empty string for other states\n\n# Create text vector for all states\ntext_all &lt;- paste(scales::comma(sunburstDF$values), sep = \" \")\n\n# generate sunburst plot\nsunburst_p &lt;- plot_ly(data = sunburstDF,\n                      ids = ~ids, \n                      labels = ~labels, \n                      parents = ~parents,\n                      values = ~values,\n                      type = 'sunburst',\n                      branchvalues = 'total',\n                      textinfo = 'label+text',   # show label and text on static chart\n                      hoverinfo = 'label+text',  # show label and text on hover\n                      text = text_selected,  # set initial text to display EV charging ports for top n states\n                      hovertext = text_all)  %&gt;%  # set hover text to display ports for all states\n  layout(title = \"Which states have the highest number of EV charging ports \\n planned and operational in the United States as of March 2024?\",\n         margin = list(t = 75, b = 75, l = 5, r = 5),\n         sunburstcolorway = c(\"West\" = \"#0c99c0\",\n                              \"Northeast\" = \"#AA4A44\", \n                              \"Southeast\" = \"#659b5e\",  \n                              \"Midwest\" = \"#ECAC00\", #ffd23f\", \n                              \"Southwest\" = \"#d3b88c\"),\n         paper_bgcolor = \"#FCF7ED\",\n         font = list(size = 14),\n         annotations = list(\n           list(\n             x = 0.5,\n             y = -0.2,  \n             xref = \"paper\",\n             yref = \"paper\",\n             text = 'Source: National Renewable Energy Laboratory (NREL).\"Alternative Fuel Stations API Documentation.\"\\n https://developer.nrel.gov/docs/transportation/alt-fuel-stations-v1',\n             showarrow = FALSE,\n             font = list(size = 12)\n           )\n         ))\n\n# display plot\nsunburst_p\n\n\n\n\nTotal number of EV charging ports installed or in progress in the United States as of March 2024\n\n\n\n\nVisualization 2: dumbbell chart\n\n\nCode\n# --- Data Wrangling --- #\n\n# --- United States --- #\n\ndac_us &lt;- us_ev_joined %&gt;% \n  select(sf, geoid10, tpf, tf_pfs, n_trn, tp_et, n_trn_eomi, sn_c) %&gt;% \n  distinct() %&gt;% \n  rename(state_name = sf,\n         is_dac = sn_c)\n\n# create a dataframe with disadvantaged communities in the US in proximity to traffic (&gt;= 90th percentile)\ndac_us_df &lt;- dac_us %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(tpf)) %&gt;% \n  filter(tpf!=0) %&gt;% \n  filter(state_name %in% selected_states) %&gt;% \n  group_by(state_name) %&gt;% \n  mutate(dac_ev_count = sum(tp_et == 1 & is_dac == 1), # count all rows where DACs are in proximity to traffic\n         dac_tot_count = sum(is_dac == 1),\n         dac_ev_tpf = sum(tpf * (tp_et == 1 & is_dac == 1)),\n         tpf_state = sum(tpf)) %&gt;% # get a total population of DACs in proximity to traffic %&gt;% \n  ungroup()\n\n# join us_fast_ports_per_state df to dac_us_df df\nus_fast_ports &lt;- us_fast_ports_per_state %&gt;% \n  select(sf, geoid10, total_ports_per_tract) %&gt;% \n  distinct()\n\nev_dac_us_df &lt;- left_join(dac_us_df, us_fast_ports, by = join_by(geoid10)) %&gt;% \n  select(-sf) %&gt;% \n  group_by(state_name) %&gt;% \n  mutate(ev_ports_per_state_pop_pct = sum(total_ports_per_tract)/tpf_state*100,\n         total_ports_state = sum(total_ports_per_tract),\n         # calculate total ports as of % of total pop of DACs suitable for EV charging\n         total_ports_dac_pop_pct = (sum(total_ports_per_tract * (tp_et == 1 & is_dac == 1))/dac_ev_tpf)*100,\n         total_ports_dac = sum(total_ports_per_tract * (tp_et == 1 & is_dac == 1))) %&gt;% \n  ungroup()\n\nev_dac_us_chart &lt;- ev_dac_us_df %&gt;% \n  select(state_name, ev_ports_per_state_pop_pct, total_ports_dac_pop_pct) %&gt;% \n  distinct()\n\n# add custom images to the dataframe\nev_state_pct_icon &lt;- \"ev_state_icon.png\"\nev_dac_pct_icon &lt;- \"ev_dac_icon.png\"\n\nev_dac_us_chart$ev_state_pct_icon &lt;- ev_state_pct_icon\nev_dac_us_chart$ev_dac_pct_icon &lt;- ev_dac_pct_icon\n\n# --- Dumbbell chart --- #\n# create a graph with points as images \ndumbell_p &lt;- ggplot(ev_dac_us_chart) +\n  geom_segment(aes(x = ev_ports_per_state_pop_pct,\n                   xend = total_ports_dac_pop_pct,\n                   y = fct_reorder(state_name, ev_ports_per_state_pop_pct),\n                   yend = state_name)) +\n  geom_image(aes(x = ev_ports_per_state_pop_pct,\n                 y = state_name, image = ev_state_pct_icon), size = 0.1) +\n  geom_image(aes(x = total_ports_dac_pop_pct,\n                 y = state_name, image = ev_dac_pct_icon), size = 0.1) + \n  labs(y = \"\",\n       x = \"% of EV Charging Ports Per Capita\",\n       title = \"Comparing EV Charger Share: State Population vs. Disadvantaged Communities\",\n       subtitle = \"Top 5 States by Number of Chargers Installed or In progress, as of March 2024\",\n       caption = 'Source: National Renewable Energy Laboratory (NREL).\"Alternative Fuel Stations API Documentation.\"\\n https://developer.nrel.gov/docs/transportation/alt-fuel-stations-v1') +\n  scale_x_continuous(labels = scales::percent_format(scale = 1), \n                     expand=c(0.1, 0)) \n    \n# add legend and theme\ndumbell_p &lt;- dumbell_p + \n  geom_rect(xmin = 0.105, xmax=0.16,\n            ymin = 0.7, ymax = 2.1,\n            fill = NA,\n            color = \"black\",\n            size = 0.05,\n            alpha=0.8) +\n  geom_image(x = 0.109, y = 1.56, image = ev_state_pct_icon) +\n  geom_image(x = 0.109, y = 1, image = ev_dac_pct_icon) +\n  annotate(\"text\", x = 0.109, y = 1.9, \n           label = \"Share of EV charging ports\", \n           size = 6,\n           hjust = 0) +\n  annotate(\"text\", x = 0.113, y = 1.56, \n           label = \"per state population\", \n           size = 5,\n           hjust = 0) +\n  annotate(\"text\", x = 0.113, y = 1, \n           label = \"per state DAC population \\nin top 10% traffic proximity\", \n           size = 5,\n           hjust = 0) +\n  guides(color = guide_legend(title = NULL)) +\n  theme_bw() +\n  theme(axis.title.x = element_blank(),\n        axis.title.y = element_text(margin = margin(r = 15), size = 16),\n        plot.title = element_text(margin = margin(b = 10), color = \"black\", size = 18),\n        plot.subtitle = element_text(margin = margin(b = 20), color = \"black\", size = 16),\n        plot.caption = element_text(margin = margin(l=0, t = 15), size = 10, color = \"black\"),\n        axis.text = element_text(size = 16, color = \"black\"),\n        panel.grid = element_blank(), \n        plot.background = element_rect(\"#FCF7ED\", color = \"transparent\"),\n        panel.background = element_rect(\"#FBFAF5\")) \n  \n# display dumbell plot\ndumbell_p\n\n\n\n\nComparing EV Charger Share: State Population vs.¬†Disadvantaged Communities\n\n\n\n\n\nVisualization: Choropleth maps\n\n\nCode\n# --- Data Wrangling --- #\nny_state &lt;- states(class = \"sf\", cb = TRUE) %&gt;%\n    clean_names() %&gt;%\n    rename(state_name = name) %&gt;%\n    filter(state_name %in% c(\"New York\"))\n\n# create EV distribution SF object projected in 'EPSG':4269\nny_ev_sf &lt;- us_ev_sf %&gt;%\n    filter(state == \"NY\") %&gt;%\n    st_transform(us_ev_sf, crs = st_crs(ny_state)) %&gt;%\n    st_make_valid()  # fix any invalid geometries after the transformation\n\n# crop ca_ev_sf df to the bounding box of New York state\nny_ev_sf &lt;- st_intersection(ny_ev_sf, ny_state)\n\n# create a sf dataframe with the locations of EV charging\n# ports\nny_ev_sf &lt;- ny_ev_sf %&gt;%\n    select(state, id, ev_level2_evse_num, ev_dc_fast_num) %&gt;%\n    unnest(cols = c(state, id, ev_level2_evse_num, ev_dc_fast_num,\n        geometry)) %&gt;%\n    filter(!(is.na(ev_level2_evse_num) & is.na(ev_dc_fast_num)))\n\nny_ev_loc &lt;- ny_ev_sf %&gt;%\n    select(id) %&gt;%\n    mutate(id = as.factor(id)) %&gt;%\n    distinct()\n\n# create DACs distribution SF object projected in\n# 'EPSG':4269\nny_dac_sf &lt;- us_ev_joined %&gt;%\n    rename(state_name = sf, is_dac = sn_c) %&gt;%\n    filter(state_name %in% c(\"New York\")) %&gt;%\n    st_transform(crs = st_crs(ny_state)) %&gt;%\n    st_make_valid()  # fix any invalid geometries after the transformation\n\n\nny_dac &lt;- ny_dac_sf %&gt;%\n    select(state_name, is_dac, tp_et, tf_pfs) %&gt;%\n    mutate(dac_type = case_when((is_dac == 1 & tf_pfs &gt;= 0.9 ~\n        \"DAC &gt; 90th perc. Traffic Proximity\"), is_dac == 1 &\n        tf_pfs &lt; 0.9 ~ \"DAC\", is_dac == 0 ~ \"Non-DAC\", .default = \"Uknown\"))\n\n\n# --- Create Map of EV Chargers distribution in New York\n# State --- #\n\n# define inset map bbox\nny_state_bbox &lt;- st_bbox(ny_state)\n\n# define bbox for the area with high concentration of DACs\n# in proximity to traffic\nny_dac_bbox &lt;- st_bbox(c(xmin = -74.2, ymin = 40.4, xmax = -73.6,\n    ymax = 41), crs = 4269)\n\n# inset map\ndac_map &lt;- ggplot() + geom_sf(data = ny_dac, aes(fill = dac_type),\n    color = \"transparent\") + scale_fill_manual(name = \" Disadvantaged Community\",\n    values = c(`Non-DAC` = \"#fff7d1\", DAC = \"#ebb3a9\", `DAC &gt; 90th perc. Traffic Proximity` = \"#861657\",\n        Uknown = \"#d9dcd6\"))\n\ndac_map &lt;- dac_map + geom_sf_text(data = ny_state, aes(label = state_name,\n    hjust = 1.5, vjust = -10.1), size = 6, color = \"grey30\",\n    family = \"sans\", fontface = \"bold\", show.legend = NA, check_overlap = FALSE) +\n    geom_sf(data = ny_ev_loc, aes(color = \"EV Charging Station\"),\n        fill = \"#a7c957\", shape = 21, stat = \"sf_coordinates\",\n        size = 1.5, alpha = 0.8) + scale_color_manual(name = \"\",\n    values = \"#386641\") + theme_void() + guides(color = guide_legend(override.aes = list(size = 4,\n    color = c(`EV Charging Station` = \"#386641\")))) + theme(plot.title = element_text(margin = margin(b = 15),\n    color = \"white\"), plot.subtitle = element_text(margin = margin(b = 10),\n    color = \"white\"), plot.caption = element_text(color = \"white\"),\n    legend.position = c(0.19, 0.188), panel.grid = element_blank(),\n    plot.background = element_rect(\"#FCF7ED\", color = \"transparent\"),\n    panel.background = element_rect(\"#FCF7ED\", color = \"transparent\")) +\n    annotate(\"rect\", xmin = ny_dac_bbox$xmin, xmax = ny_dac_bbox$xmax,\n        ymin = ny_dac_bbox$ymin, ymax = ny_dac_bbox$ymax, fill = \"transparent\",\n        color = \"#861657\", linewidth = 1)\n\n# zoom in on the area with high DACs consentration\n\nny_ev_loc_zoom &lt;- ny_ev_loc %&gt;%\n    st_crop(ny_dac_bbox)\n\nny_dac_zoom &lt;- ny_dac %&gt;%\n    st_crop(ny_dac_bbox)\n\ndac_zoom_map &lt;- ggplot() + geom_sf(data = ny_dac_zoom, aes(fill = dac_type),\n    color = \"grey85\") + scale_fill_manual(name = \" Disadvantaged Community\",\n    values = c(`Non-DAC` = \"#fff7d1\", DAC = \"#ebb3a9\", `DAC &gt; 90th perc. Traffic Proximity` = \"#861657\",\n        Uknown = \"#d9dcd6\"))\n\ndac_zoom_map &lt;- dac_zoom_map + geom_sf(data = ny_ev_loc_zoom,\n    aes(color = \"EV Charging Station\"), fill = \"#a7c957\", shape = 21,\n    stat = \"sf_coordinates\", size = 1.5, alpha = 0.9) + scale_color_manual(name = \"\",\n    values = \"#386641\") + theme_void() + theme(legend.position = \"none\",\n    panel.background = element_rect(fill = \"#FBFAF5\", color = \"#861657\",\n        linewidth = 1), plot.background = element_rect(fill = \"#FCF7ED\"))\n\n\ndac_map &lt;- dac_map + coord_sf(xlim = c(-81, -65), ylim = c(39,\n    46), expand = FALSE, clip = \"off\")\n\n# combine maps\nchoropleth_p &lt;- ggdraw() + draw_plot(dac_map, x = -0.04, y = 0.01,\n    width = 1, height = 1) + draw_plot(dac_zoom_map, x = 0.455,\n    y = 0.19, width = 0.6, height = 0.6)\n\n\n\n\n\n\n\nA distribution of EV chargers in New York relative to disadvantaged communities as of March 2024"
  }
]